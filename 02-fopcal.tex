\setcounter{chapter}{1}
\chapter{First-Order Predicate Logic}

In this chapter,
we'll review the syntax and semantics of first-order predicate logic.
In contrast to propositional logic,
most
(some would say: all)
mathematical reasoning can be formalized in first-order logic.

\section{Syntax}\label{sec:syntax1}

I'll begin with a basic version of first-order logic,
without function symbols and identity;
these will be added in section \ref{sec:functions}.
For now,
the \emph{primitive symbols} of a first-order language $\L_1$
therefore fall into
the following categories
(whose members must not be part of one another):
%
\begin{itemize*}
\item a countably infinite set of \textit{(individual) variables},
\item a countably infinite set of \textit{(individual) constants},
\item for each natural number $n$, a set of \textit{$n$-ary predicate symbols},
\item the \textit{connectives} `$\neg$' and `$\to$',
\item the \textit{universal quantifier symbol} `$\forall$',
\item the parentheses `(' and `)'.
\end{itemize*}

The individual constants and variables constitute the \emph{singular terms} of $\L_1$.
Intuitively,
their function is to pick out an object,
which might be a person, a number, a set, or anything else.
Predicate symbols are used to attribute properties or relations to these objects.
For example,
we might have individual constants `$a$' and `$b$' for Athens and Berlin
and a binary predicate `$R$' for \emph{being west of}.
`$Rab$' would then state that Athens is west of Berlin,
and `$Rba$' that Berlin is west of Athens.
The predicate symbol always comes first.

From \emph{atomic sentences} like `$Rab$' or `$Fa$',
we can form complex sentences in the familiar way
with the help of `$\neg$', `$\to$', and the parentheses:
$\neg Rab$, \, $(Rab \to Fa)$, \, $\neg(Rab \to Fa)$, etc.

The real power and complexity of first-order logic comes from
its quantificational apparatus.
The quantifier symbol `$\forall$' allows making general claims about all objects
-- where by `all' I mean
all objects in the intended domain of discourse.
In a formal theory of arithmetic,
for example,
the intended domain of discourse would consist of the natural numbers 0, 1, 2, 3, etc.
It would not include Athens.
In this context,
`$\forall x Fx$' would state that
every natural number has the property expressed by `$F$'.

Some practice is required to become familiar with the use of `$\forall$',
as it has no direct analog in natural language.
The closest translation of `$\forall x Fx$' in English is something like

\begin{quote}
  Everything is such that it is $F$.
\end{quote}

This can obviously be simplified to `Everything is $F$';
but in that sentence,
`everything' combines directly with a predicate (`is $F$'),
whereas `$\forall x$' combines with an expression of sentential type, `$Fx$'.
The variable `$x$' works much like the pronoun `it' in English.
Overt variables are sometimes used in English when quantifiers are nested:

\begin{quote}
  For every number $x$ there is a number $y$ greater than $x$ such that every
  number greater than $y$ is greater than $x$.
\end{quote}

This can be easily expressed in first-order logic:

\[
  \forall x \exists y (Gyx \land \forall z (Gzy \to Gzx)).
\]

\begin{definition}{}{formula}
  A \emph{formula} of a basic first-order language $\L_1$
  is a finite string built up according to the following formation rules:
  \begin{enumerate*}
    \item[(i)] If $P$ is an $n$-ary predicate symbol of $\L_1$ and $t_{1},\ldots,t_{n}$ are singular terms of $\L_1$ then $Pt_{1}\ldots t_{n}$ is a formula.
    \item[(ii)] If $A$ is an $\L_1$-formula, then so is $\neg A$.
    \item[(iii)] If $A$ and $B$ are $\L_1$-formulas, then so is $(A \to B)$.
    \item[(iv)] If $x$ is a variable and $A$ is a formula of $\L_1$ then $\forall x A$ is a formula.
  \end{enumerate*}
\end{definition}
\noindent%
Here, `$P$', `$t_{1}$', `$t_{n}$', `$A$', `$B$', `$x$' are metalinguistic variables
standing for expressions in $\L_1$.
I haven't specified what the predicate symbols, individual constants, and variables of
the object language look like.

As in the case of propositional logic,
we introduce some shortcuts in the metalanguage, writing

\begin{itemize*}
\item $(A \land B)$ for $\neg(A \to \neg B)$;
\item $(A \lor B)$ for $(\neg A \to B)$;
\item $(A \leftrightarrow B)$ for $\neg ((A \to B) \to \neg(B \to A))$.
\item $\top$ for $A \to A$;
\item $\bot$ for $\neg(A \to A)$;
\item $\exists x A$ for $\neg \forall x \neg A$.
\end{itemize*}
\noindent%
The last of these is new.
We'll omit parentheses and quotation marks when no ambiguity threatens.

Definition \ref{def:formula} allows for formulas like these:
%
\begin{gather*}
  Rax\\
  Fa \to Gx
\end{gather*}
%
If, as before, we interpret
`$a$' as denoting Athens and `$R$' as being west of,
`$Rax$' could be read as `Athens is west of $x$'.
But variables, unlike constants, don't pick out a definite object.
Their only function is to construct quantified statements.
`$\forall x Rax$' would say that Athens is west of everything,
but `$Rax$' doesn't really say anything.
It is neither true nor false.
% (We might say that it is true ``relative
% to some interpretations of `$x$'\,'' and false relative to others. For example,
% it is true if we interpret `$x$' as picking out 1, and false if we interpret it
% as picking out 0.)

Formulas like `$Rax$' and `$Fa \to Gx$' that contain a variable
without a matching quantifier are called ``open''.
Formulas without such variables are ``closed''.
Only closed formulas make a genuine claim about
the intended domain of discourse.

Let's make this distinction more precise.
A \emph{quantifier} consists of the symbol `$\forall$' followed by a variable.
That variable is said to be \emph{bound} by the quantifier.
Next,
define a \emph{subformula} of a formula as any part of the formula that is itself a formula.
For example,
$Fx$ is a subformula of $\forall x Fx$.
The \emph{scope} of an occurrence of a quantifier $\forall x$ in a formula is
the shortest subformula that contains the occurrence.
So the scope of $\forall x$ in $Fa \lor \forall x (Fx \to Gy)$ is
$\forall x(Fx \to Gy)$.
An occurrence of a variable in a formula is \emph{bound}
if it lies in the scope of an occurrence of a quantifier that binds it.
An occurrence of a variable that isn't bound is \emph{free}.
A formula in which some variable occurs free is \emph{open}.
A formula that isn't open is \emph{closed}.
A \emph{sentence} is a closed formula.

\begin{exercise}
  Why do I say that "occurrences" of a variable in a formula are free or bound?
  Why not simply say that a variable is free or bound in a formula?
\end{exercise}

\begin{exercise}
  Assume that `$F$' is a 1-ary (= \emph{monadic}) predicate, `$a$' is a constant, and `$y$' a variable.
  Which of the following are formulas?
  Which are sentences?
  Mark the scope of each quantifier.

  (a) $Fa \to \forall x Fx$\quad (b) $\forall x Fx \to Fx$\quad (c) $\forall x Fa$\quad (d) $\forall x (Fa \to \forall x \neg (Fx \to Fa))$
\end{exercise}

% \begin{exercise}
%   $\L_{1}$ \textit{extends} $\L_{0}$ in the sense that every $\L_{0}$-sentence
%   is also an $\L_{1}$-sentence. Explain why this is true.
% \end{exercise}

\section{The first-order predicate calculus}

Frege's \emph{Begriffsschrift} from 1879
contains a complete proof system for first-order logic.
However,
the formal language of the \emph{Begriffsschrift} is not a first-order language,
as it allows quantifying into predicate position.
In Frege's language,
one can say not only things like `$\forall x\, Fx$',
but also `$\forall X\, Xa$',
which (roughly) means that $a$ has every property.
Quantifiers that bind predicate-type expressions are called \emph{second-order},
and the resulting logic is called \emph{second-order logic}.
We'll take a closer look at second-order logics in chapter~\ref{ch:hol}.

A complete calculus for pure first-order logic was first presented
by David Hilbert and Wilhelm Ackermann in 1928.
I give a slightly simplified version of their calculus,
which I'll call \emph{the first-order predicate calculus}.

Like the propositional calculus from the previous chapter,
the first-order calculus consists of some axioms and inference rules.
In fact,
we'll take over all the axioms and rules of the propositional calculus.
All instances of A1-A3 are axioms,
and Modus Ponens (MP) is a rule of our new calculus.
To these,
we add some principles for dealing with quantifiers.
Let's think about what we need.

A common inference pattern in first-order logic is
``universal instantiation'':
having shown that \emph{every} object has some property,
we infer that a particular object $c$ has that property.
To state this precisely,
we need some notation for substitution.
If $A$ is a formula,
$x$ a variable, and
$c$ an individual constant,
I write `$A(x/c)$' for the formula
obtained from $A$ by
replacing all free occurrences of $x$ in $A$ with $c$.
For example,
`$Fx(x/a)$' denotes the formula `$Fa$',
but `$\forall x Fx(x/a)$' denotes `$\forall x Fx$'
rather than the nonsensical `$\forall a Fa$'.
In informal contexts,
I'll often write `$A(x)$' to indicate that
$A$ is a formula in which the variable $x$ occurs freely;
`$A(t)$' is then shorthand for `$A(x/t)$'.

\begin{exercise}
  Let $A$ be $\forall x (Fx \to Gy) \to  \forall y Fy$. What is $A(y/b)$?
\end{exercise}

We can now formulate a rule of universal instantiation:
if $A$ is a formula, $x$ a variable and $c$ a constant,
one may infer $A(x/c)$ from $\forall x A$.
We won't actually add this as a new rule, however,
because we can just as well add a corresponding axiom schema:
%
\begin{axioms}
  A4 & $\forall x A \to A(x/c)$
\end{axioms}
%
\noindent%
Given A4, we can use MP to reason from $\forall x A$ to $A(x/c)$.

We introduce a genuine rule -- called \emph{(Universal) Generalization} -- for the
inference in the opposite direction,
from a particular case to a general claim:

\begin{axioms}
    Gen & $\text{From }A \text{ one may infer }\forall x A(c/x).$
\end{axioms}

% Careful: This isn't sound: From A(x/c) one may infer \forall x A.
% Counterexample: From a=a to \forall x x=a.

\noindent%
Here,
`$(c/x)$' expresses the inverse of `$(x/c)$':
$A(c/x)$ is the formula
obtained from $A$ by
replacing all occurrences of $c$ in $A$ with $x$,
except for any occurrences that would let $x$ become bound.
(Just as $A(x/c)$ only replaces free occurrences of $x$,
we want $A(c/x)$ to only create free occurrences of $x$.)

The Gen rule requires explanation.
Do we really want to infer $\forall x Fx$ from $Fa$?
The inference clearly isn't valid:
it's easy to imagine cases where a particular object $a$ is $F$,
but other objects are not $F$.
But remember that each line in a strictly Hilbert-style axiomatic proof
is either an axiom or follows from an axiom by an inference rule.
None of our axioms or rules will allow
making specific claims about any particular object
that one couldn't equally make about all other objects.
$Fa$ won't be provable.
The only provable sentences involving individual constants will be
logical truths like $Fa \to Fa$.
And here,
the inference to $\forall x (Fx \to Fx)$ is safe.

To get a complete calculus,
we need one more axiom schema:
%
\begin{axioms}
  A5 & $\forall x(A \to B) \to (A \to \forall x B), \text{ if $x$ is not free in $A$}$
\end{axioms}
%
\noindent%
To see the point of this,
suppose that in the course of a proof we have
established the following claims,
for some $A$ and $B(x)$,
where $x$ isn't free in $A$:
%
\begin{quote}
  $\forall x (A \to B(x))$ \\
  $A$
\end{quote}
%
\noindent%
If we had the rule of universal instantiation,
we could deduce $A \to B(c)$ from the first line,
then use MP to infer $B(c)$ and finally infer $\forall x B(x)$ by Gen.
This reasoning can't be replicated
after we've replace universal instantiation by the axiom schema A4.
So we add A5,
which allows inferring $\forall x B(x)$ by two applications of Modus Ponens.

Here's a summary of our axioms and rules:

\begin{axioms}
A1 & $A \to (B \to A)$\\
A2 & $(A \to (B \to C)) \to ((A \to B) \to (A \to C))$\\
A3 & $(\neg A \to \neg B) \to (B \to A)$\\
A4 & $\forall x A \to A(x/c)$\\
A5 & $\forall x(A \to B) \to (A \to \forall x B), \text{ if $x$ is not free in $A$}$\\
MP & $\text{From }A\text{ and }A \to B\text{ one may infer }B.$\\
Gen & $\text{From }A \text{ one may infer }\forall x A(c/x).$
\end{axioms}

\vspace{-\baselineskip}
\begin{definition}{Proof}{proof0}
  A \emph{proof} of a sentence $A$ in the first-order calculus is a finite sequence of sentences $A_{1},A_{2},\ldots A_n$ with $A_n = A$,
  such that each $A_i$ is
  either an instance of A1-A5 or follows from earlier sentences in the sequence by MP or Gen.
\end{definition}

\noindent%
I'll use `$\vdash A$' to express that $A$ is provable in the first-order
calculus.

As in the case of propositional logic,
it is convenient to generalize our Hilbert-style proof system to
allow for deductions from premises.
In this case,
we have to restrict the use of Gen:
we don't want to infer $\forall x Fx$ from $Fa$.

\begin{definition}{}{deduction1}
  If $A$ is a first-order sentence and $\Gamma$ a set of first-order sentences,
  a \emph{deduction} of $A$ from $\Gamma$ in the first-order calculus is
  a finite sequence of sentences $A_{1}, A_{2}, \ldots A_n$,
  with $A_n = A$,
  such that each $A_i$ is
  either an instance of A1--A5,
  an element of $\Gamma$,
  or follows from previous sentences by MP or Gen,
  but without applying Gen to an individual constant $c$ that
  occurs in one of the sentences in the sequence that are elements of $\Gamma$.
  % Why not: ... that occurs in $\Gamma$?
  % Because then Gen can never be applied if $\Gamma$ contains every constant,
  % even if it would be fine
  % to apply it because the relevant $c$ was actually introduced by A4.
  % I suspect this would break completeness.
  % I noticed it in the proof of lemma \ref{lem:consistency-in-Lplus},
  % which doesn't go through if we use the simpler restriction.
\end{definition}

We write `$\Gamma \vdash A$' to express that there is a deduction of $A$ from $\Gamma$.
Let's investigate what this relation looks like.

The structural principles Id, Mon, and Cut from the previous chapter hold
for every axiomatic calculus.
So we have
%
\begin{axioms}
Id & $A \vdash A$\\
Mon & $\text{If }\Gamma \vdash A\text{ then }\Gamma, B \vdash A$\\
Cut & $\text{If }\Gamma \vdash A\text{ and }\Delta,A \vdash B\text{ then }\Gamma,\Delta \vdash B$
\end{axioms}
%
\noindent%
(As in the previous chapter,
we generally omit set brackets on the left-hand side of `$\vdash$',
and write a comma to indicate unions:
`$\Gamma, B \vdash A$' is shorthand for `$\Gamma \cup \{ B \}\vdash A$'.)

The Deduction Theorem also still holds:

\begin{theorem}{The Deduction Theorem (DT)}{deduction-theorem1}
  If $\Gamma,A \vdash B$ then $\Gamma \vdash A \to B$.
\end{theorem}

\begin{proof}
  \emph{Proof.}
  Let $B_{1}, B_{2}, \ldots, B_{n}$ be a deduction of $B$ from $\Gamma \cup \{ A \}$.
  We prove by strong induction on $k$ that
  $\Gamma \vdash A \to B_{k}$ for all $k = 1, 2, \ldots, n$.
  That is,
  we show that
  \emph{if} $\Gamma \vdash A \to B_{i}$ for all $i < k$,
  \emph{then} $\Gamma \vdash A \to B_{k}$.

  We need to distinguish four cases,
  corresponding to the ways in which $B_{k}$ can appear in the deduction:
  as an axiom,
  as an element of $\Gamma \cup \{ A \}$,
  from an application of MP,
  or from an application of Gen.
  The proof for the first three cases is
  exactly as in the proof of the Deduction Theorem for the propositional calculus.
  It remains to check the case of Gen.

  Assume $B_{k}$ follows from $B_{i}$ by an application of Gen.
  So $B_{k}$ is of the form $\forall x B_{i}(c/x)$,
  and $c$ doesn't occur in $\Gamma$ or $A$.
  By induction hypothesis,
  there is a deduction of $A \to B_{i}$ from $\Gamma$.
  As $c$ doesn't occur in $\Gamma$,
  we can apply Gen,
  getting a deduction of $\forall x (A \to B_{i})(c/x)$.
  Since $c$ doesn't occur in $A$,
  this formula can also be written as $\forall x (A \to B_{i}(c/x))$.
  A5 gives us
  $\forall x (A \to B_{i}(c/x)) \to (A \to \forall x B_{i}(c/x))$.
  By MP,
  we therefore get a deduction of $A \to \forall x B_{i}(c/x)$ from $\Gamma$.
  \qed
\end{proof}

You may remember that
we don't need to invoke A1 and A2 any more
once we have DT and MP.
Similarly,
once we have DT, MP, and Gen,
we no longer need A5,
as any instance of it can be derived.
Here is how.

Assume, as in the statement of A5,
that $x$ is not free in $A$.
Let $c$ be a constant that doesn't occur in $A$ or $B$.
Then:

\vspace{-\baselineskip}
\begin{flalign*}
  \quad 1.\quad & \forall x (A \to B), A \vdash \forall x (A \to B) && \text{(Id, Mon)} & \\
  \quad 2.\quad & \vdash \forall x (A \to B) \to (A \to B(x/c)) && \text{(A4, $x$ not free in $A$)} & \\
  \quad 3.\quad & \forall x (A \to B), A \vdash A \to B(x/c) && \text{(MP, 1, 2)} & \\
  \quad 4.\quad & \forall x (A \to B), A \vdash A && \text{(Id)} & \\
  \quad 5.\quad & \forall x (A \to B), A \vdash B(x/c) && \text{(MP, 3, 4)} & \\
  \quad 6.\quad & \forall x (A \to B), A \vdash \forall x B && \text{(Gen, 5, $B(x/c)(c/x) = B$)} & \\
  \quad 7.\quad & \forall x (A \to B) \vdash A \to \forall x B && \text{(DT, 6)} & \\
  \quad 8.\quad & \vdash \forall x (A \to B) \to (A \to \forall x B) && \text{(DT, 7)} &
\end{flalign*}

From A4 and DT, we get the rule of universal instantiation:

\begin{theorem}{Universal Instantiation (UI)}{ui}
  If $\Gamma \vdash \forall x A$ then $\Gamma \vdash A(x/c)$.
\end{theorem}
\begin{proof}
  \emph{Proof.}
  Assume $\Gamma \vdash \forall x A$.
  By A4,
  $\vdash \forall x A \to A(x/c)$.
  So by MP,
  $\Gamma \vdash A(x/c)$.
  \qed
\end{proof}

From this (and DT),
we can derive any instance of A4.
So we won't need to invoke A4 any more.

The derivations of EFQ, DNE, and RAA from the previous chapter all go through as before,
and make any appeal to A3 unnecessary.

In fact,
we know from the completeness theorem for propositional logic that
all truth-functional tautologies are provable from A1--A3 and MP.
A \emph{truth-functional tautology} is a sentence that is true on every truth-value assignment to atomic sentences.
For example,
$Fa \to Fa$ is a truth-functional tautology,
and so is $\neg\neg \forall x Fx \to \forall x Fx$.

\begin{theorem}{Tautologies (Taut)}{taut1}
  $\vdash A$ whenever $A$ is a truth-functional tautology.
\end{theorem}
\begin{proof}
  \emph{Proof.}
  Consider the propositional language $\L_0$ whose ``sentence letters'' are
  the atomic sentences of the first-order language.
  By theorem \ref{thm:completeness0},
  (the completeness theorem for propositional logic),
  every sentence in this language that is true on every truth-value assignment
  is provable from A1--A3 and MP.
  \qed
\end{proof}

% Let's summarize the key features of the classical first-order calculus:

% \begin{axioms}
%   Id & $A \vdash A$\\
%   Mon & $\text{If }\Gamma \vdash A\text{ then }\Gamma, B \vdash A$\\
%   Cut & $\text{If }\Gamma \vdash A\text{ and }\Delta,A \vdash B\text{ then }\Gamma,\Delta \vdash B$\\
%   MP & If $\Gamma \vdash A$ and $\Gamma \vdash A \to B$ then $\Gamma \vdash B$.\\
%   DT & If $\Gamma,A \vdash B$ then $\Gamma \vdash A \to B$.\\
%   Taut& $\Gamma \vdash A$ if $A$ is a truth-functional tautology.\\
%   Gen & If $\Gamma \vdash A$ and $c$ doesn't occur in $\Gamma$ then $\Gamma \vdash \forall x A(c/x)$.\\
%   UI & If $\Gamma \vdash \forall x A$ then $\Gamma \vdash A(x/c)$.
% \end{axioms}

As in the case of propositional logic,
we could use the facts that we have established about $\vdash$:
Id, Mon, Cut, DT, UI, Taut, together with MP and Gen,
to define a sequent calculus.
From this,
we could derive the kind of natural deduction or tableau calculus
that you have probably learned in your intro logic course.
We won't pause to explore these matters.

\begin{exercise}
  Show that if $\Gamma\vdash A(x/c)$ then $\Gamma \vdash \exists x A$.
\end{exercise}

\begin{exercise}
  Show that if $\Gamma \vdash \forall x (A \to B)$ then $\Gamma \vdash \forall x A \to \forall x B$.
\end{exercise}

% \begin{exercise}\label{ex:vdash0-examples} xxxxxxx
%   Show:
%   (a) $\neg A \vdash A \to B$. % needed in completeness
%   \quad (b) $B \vdash A \to B$. % needed in completeness
%   \quad (c) $A \to \neg A \vdash \neg A$; % Bostock 211
%   % \quad (b) $A \vdash \neg\neg A$; % dito
% \end{exercise}

% Side note: I have stipulated that each line in a proof must be a sentence. In
% some versions of the first-order calculus, open formulas are allowed in proofs:
% from $\forall x Fx$, one may infer $Fx$; the variable indicates that $x$ is an
% ``arbitrary individual''. The same \emph{sentences} are provable in either
% version of the calculus. Since individual constants aren't used in reasoning
% from universal formulas, the alternative calculus doesn't need my assumption
% that $\L_{1}$ has an unbounded supply of such constants. End of side note.

\section{Semantics}\label{sec:semantics1}

I've already explained informally how first-order languages are interpreted:
individual constants are assumed to pick out objects in the intended domain of discourse;
predicate symbols express properties or relations among these objects.
We'll now make this more precise.

Our semantics is inspired by the truth-conditional approach to meaning.
Plausibly,
we can determine the conditions under which an atomic first-order sentences is true
by assigning objects to individual constants,
and properties or relations to predicate symbols.
For example,
if we know that
`$a$' picks out Athens,
`$b$' Berlin,
and `$R$' the property of being west of,
we can determine that
`$Rab$' is true in a possible scenario iff Athens is west of Berlin in that scenario.

% We don't need to worry much about the concept of an ``object''. We'll assume
% that in any use of a first-order language, there is an intended domain of things
% we want to talk about; each such thing will qualify as an ``object'' -- as a
% candidate denotation of individual constants. But what is a property (or
% relation)?
%
% We can work backwards from the meaning of sentences. The meaning of `$Fa$' is
% supposed to determine, for each conceivable scenario, whether `$Fa$' is true or
% false in that scenario. Given that `$a$' can pick out any object in the intended
% domain, the meaning of `$F$' must determine a truth-value for each object (in
% the domain) and each scenario.
%
% Maybe cut this and directly state that we can make this simpler and more precise
% if (a) we deal with mathematical interpretations, and (b) if our main interest
% is in logical validity and entailment.

Now remember that logic abstracts away from the meanings of non-logical expressions.
Some premises logically entail a conclusion
iff there is no conceivable scenario in which
the premises are true and the conclusion false,
\emph{under any interpretation of the non-logical vocabulary}.
The non-logical parts of a first-order language are
its individual constants and predicate symbols.
As in the case of propositional logic,
we will define a \emph{model} as a structure
that contains just enough information about
a scenario and an interpretation of the non-logical vocabulary
to determine the truth-values of all sentences.

What do you need to know about a scenario $S$ and an interpretation $I$
to figure out whether, say, $Rab$ is true?
It would obviously suffice to know
(1) which objects are picked out by `$a$' and `$b$' under $I$,
(2) which relation is expressed by `$R$' under $I$,
and (3) whether that relation holds between those two objects in $S$.
But you don't need all that information.
It would also suffice to know
(1) which objects are picked out by `$a$' and `$b$' under $I$,
and (2) which pairs of objects in $S$ stand in the relation expressed by `$R$' under $I$.
For example,
if I told you that
`$a$' picks out Athens,
`$b$' Berlin,
and `$R$' expresses a relation that holds between all and only the following pairs of objects: $\t{\text{Athens, Berlin}}$, $\t{\text{Berlin, Paris}}$, $\t{\text{Paris, Rome}}$,
you'd know enough to figure out that `$Rab$' is true
-- although you don't really know what the sentence says
or what the scenario is like.

\begin{definition}{}{model1}
  A \emph{model} $\Mod{M}$ of a first-order language $\L_{1}$ consists of
  \begin{cenumerate}
    \item[(i)] a non-empty set $D$, called the \emph{domain} or \emph{universe} of $\Mod{M}$, and
    \item[(ii)] an \emph{interpretation function} $I$ that assigns
      to each individual constant of $\L_{1}$ a member of $D$, and
      to each $n$-ary predicate of $\L_{1}$ a set of $n$-tuples from $D$.
  \end{cenumerate}
\end{definition}

An ``$n$-tuple'' is a list of $n$ objects.
A 1-tuple is simply an object.
So a ``set of 1-tuples from $D$'' is a set of members of $D$
a ``set of 2-tuples from $D$'' is a set of pairs of members of $D$,
and so on.
The set assigned to a predicate is called the \emph{extension} of the predicate.

% The set of objects that have a property is called the property's \emph{extension};
% similarly, a relation's extension is
% the set of tuples of objects that stand in the relation.
% A first-order model assigns to each predicate an extension.

A model's domain can be arbitrarily large, but it can't be empty.
That's because I've stipulated that
every first-order language has infinitely many individual constants,
and definition~\ref{def:model1} requires that
every such constant be assigned an object in the domain.
This wouldn't be possible if the domain were empty.
But a single object is enough because
we allow that all constants pick out the same object.

% As a general theory of meaning, this is clearly inadequate. After all, there are
% conceivable scenarios in which Kurt Gödel is not a logician, or in which, say,
% Carla Bruni is a logician; but there is no conceivable scenario in which Gödel
% isn't in the set $\{ Kurt Gödel, \ldots \}$ or in which Carla Bruni is in that
% set. There are two reasons why we can nonetheless largely get away with the
% ``extensional'' account of predicate meaning.

% The first is that many logicians are mostly interested in mathematical
% interpretations, where we might read `$a$' as denoting the number 2 and `$F$' as
% expressing the property of being prime. While Kurt Gödel could have failed to be
% a logician, the number 2 could not have failed to be prime: there is no
% conceivable scenario in which 2 isn't prime; nor is there a conceivable scenario
% in which, say, the number 4 is prime. In general, it is reasonable to hold that
% purely mathematical truths don't vary across conceivable scenarios. In this
% special case, truth-conditions are determined by extensions.

It is useful to have an expression for the denotation of a non-logical symbol $s$ in a model $\Mod{M}$.
I'll use `$\llbracket s \rrbracket^{\Mod{M}}$'.
That is,
if $\Mod{M}$ is a model with interpretation function $I$,
$c$ is an individual constant and $P$ a predicate,
then $\llbracket c \rrbracket^{\Mod{M}}$ is $I(c)$
and $\llbracket P \rrbracket^{\Mod{M}}$ is $I(P)$.

\begin{exercise}
  We can mimic sentence letters
  by using zero-ary predicate symbols.
  For example,
  if $P$ and $Q$ are zero-ary predicates,
  then $P \to Q$ is a sentence.
  We might expect that a model should assign a truth-value to zero-ary predicates.
  How can we define the truth-values $T$ and $F$ to get this result
  out of definition~\ref{def:model1}?
  (Hint:
  A \emph{0-tuple} is a list of zero objects.
  There is only one such list: the empty list.)
\end{exercise}

Next,
we define what
it takes for a sentence $A$ to be true in a model $\Mod{M}$.
For atomic sentences,
this is easy.
`$Rab$', for example, is true in $\Mod{M}$ iff
the pair of objects assigned (by $\Mod{M}$) to `$a$' and `$b$'
are in the set assigned to `$R$'.
For negated sentences and conditionals,
we can use the same clauses as in propositional logic.
Quantified sentences require a little more thought.

Let $A(x)$ be some formula in which $x$ is free.
Under what conditions is $\forall x A(x)$ true in a model $\Mod{M}$?
As a first shot,
one might suggest that
$\forall x A(x)$ is true iff $A(c)$ is true for every individual constant $c$.
This is called a \textit{substitutional interpretation} of the quantifier.
It assumes that
every object in the domain is picked out by some individual constant.
Definition~\ref{def:model1} doesn't guarantee this.
It allows for models in which
some objects don't have a name,
just as most stars and most real numbers don't have a name in English.

What we'll say instead is that
$\forall x A(x)$ is true in a model $\Mod{M}$ iff
$A(c)$ is true in every model
that differs from $\Mod{M}$ at most in the object it assigns to $c$,
where $c$ is some individual constant that doesn't already occur in $A(x)$.
For example,
`$\forall x Rax$' is true in $\Mod{M}$
iff `$Rab$' is true in every model that differs from $\Mod{M}$ at most in the object it assigns to `$b$'.
By varying the interpretation of `$b$',
we can check whether $a$ stands in $R$ to every object in the domain.
For definiteness,
we'll say that $c$ is the ``alphabetically first'' individual constant
that doesn't occur in $A(x)$,
assuming that the constants come with some alphabetical order.

This approach to the semantics of quantifiers goes back to Benson Mates.
An equally popular alternative,
due to Alfred Tarski,
states that
$\forall x A(x)$ is true in a model $\Mod{M}$ iff
$A(x)$ is true for every way of assigning an individual to $x$.
This requires defining a truth relation
not just between sentences and models,
but between sentences, models, and so-called ``assignment functions''
that assign objects to variables.
The two approaches deliver the same results.
I use Mates' because it requires slightly less machinery.

% Think of the open sentence $\phi(x)$ as possibly \textit{true of} some objects and false
% of others. $\forall x \phi(x)$ is true iff $\phi(x)$ is true of all objects.

\begin{definition}{}{satisfaction1}
  An $\L_{1}$-sentence $A$ is true in a model $\Mod{M}$ (for short, $\Mod{M} \satisfies A$)
  iff one of the following conditions holds.
  \begin{cenumerate}
    \item[(i)] $A$ is an atomic sentence $Pc_{1}\ldots c_{n}$ and
      $\t{\llbracket c_{1} \rrbracket^{\Mod{M}}, \ldots, \llbracket c_{n} \rrbracket^{\Mod{M}}} \in \llbracket P \rrbracket^{\Mod{M}}$.
    \item[(ii)] $A$ has the form $\neg B$ and $\Mod{M} \not\satisfies B$.
    \item[(iii)] $A$ has the form $(B \to C)$ and $\Mod{M} \not\satisfies B$ or $\Mod{M} \satisfies C$.
    \item[(iv)] $A$ has the form $\forall x B$ and $\Mod{M}' \satisfies B(x/c)$ for every
      model $\Mod{M}'$ that differs from $\Mod{M}$ at most in the object assigned to $c$,
      where $c$ is the alphabetically first individual constant that does not occur in $B$.
  \end{cenumerate}
\end{definition}
%
\noindent%
If $A$ is true in $\Mod{M}$,
we also say that $\Mod{M}$ is a model of $A$,
or that $\Mod{M}$ satisfies $A$,
A model satisfies a set of sentences if it satisfies each sentence in the set.

Entailment and validity are defined in terms of satisfaction,
as in the previous chapter.

\begin{definition}{}{entailment1a}
  A set of sentences $\Gamma$ \emph{entails} a sentence $A$
  (for short, $\Gamma \entails A$)
  iff every model that satisfies $\Gamma$ also satisfies $A$.

  Sentences $A$ and $B$ are \emph{equivalent} if $A \entails B$ and $B \entails A$.

  A sentence is \emph{valid} (for short, $\entails A$) iff it is satisfied by every model.
\end{definition}

\begin{exercise}
  State the truth conditions for $\exists x A$.
  That is, complete this sentence:
  `$\exists x A$ is true in a model $\Mod{M}$ iff \ldots'.
\end{exercise}

% \begin{exercise}
%   Show that $\Gamma, A \entails B$ iff $\Gamma \entails A \to B$.
% \end{exercise}

\begin{exercise}
  Give a countermodel to show that
  $\forall x (Fx \lor Gx) \not\entails \forall x Fx \lor \forall x Gx$.
\end{exercise}

\begin{exercise}
  Show that if $x$ is not free in $B$ then
  $\forall x(A \to B)$ is equivalent to $\exists x A \to B$.
\end{exercise}

% \begin{exercise}
% Show that substituting a subformula by an equivalent subformula results in an equivalent formula.
% \end{exercise}

% ** Prenex normal form

% \begin{definition}{Prenex Normal Form}{prenex}
% A formula is in \emph{prenex normal form} if all its quantifiers are at the
% beginning.
% \end{definition}

% The idea of the proof is to demonstrate that a quantifier which occurs
% somewhere in the middle of a formula can always be moved one step to the
% left, and by sufficiently many such steps we can bring all the quantifiers as far
% to the left as possible, so that they do all occur in a block at the beginning.

% All we need to show is that
% \begin{itemize}
% \item A → ∀xB is equivalent to ∀x(A → B), if x is not free in A.
% \item A → ¬∀xB is equivalent to ¬∀x(A → B), if x is not free in A.
% \item alpha-equivalence
% \end{itemize}

% Maybe show semantically?

% Needed to generalize $\Sigma$ and $\Pi$: Call a formula of LPA essentially $\Sigma_n$ or $\Pi_n$ if it can be transformed into a $\Sigma_n$ or
% a $\Pi_n$ formula, respectively, by the usual prenexing rules.

\section{Functions and identity}\label{sec:functions}

Consider the sentence `$1+2=3$'.
How could we translate this into a first-order language?
We could use a three-place predicate symbol $S$ and write `$S(1,2,3)$'.
But this isn't ideal.
It obscures the structure of the original sentence,
which states an identity between $1+2$ and $3$.

As a first step to remedy this situation,
let's introduce a predicate for identity.
We'll use `='.
So `$=\!ab$' states that $a$ equals $b$,
in the sense that $a$ and $b$ are the very same object.
For legibility,
we'll ``abbreviate'' this as `$a=b$'.
We'll also write `$a\not= b$' for `$\neg =\!ab$'.

Of course,
nothing in our earlier definition of first-order languages
prevented us from having a predicate `='.
The real novelty is that
we now classify `=' as a logical expression.
This means that its interpretation is held fixed:
in every model,
`=' is interpreted as the identity relation (on the model's domain).
We'll also introduce new rules for reasoning with `='.
Before we get to these changes,
I want to introduce another addition to our definition of first-order languages
that allows forming complex terms like `$1+2$'.

Let's think about how such terms work.
The expression `$1+2$' denotes a number:
the number 3.
(That's why `$1+2 = 3$' is true.)
In general,
for any numerical terms `$a$' and `$b$',
`$a+b$' denotes a number:
the sum of $a$ and $b$.
We can therefore understand the `$+$' sign as expressing
a function that maps a pair of numbers to their sum.
So understood,
`$+$' is a \emph{function symbol}.
Function symbols are ubiquitous in maths.
It's useful to have them in our formal languages as well.

Let me say a few general words on the concept of a function,
as it will play an important role throughout these notes.
A function, in the mathematical and logical sense,
takes one or more objects as input and (typically) returns an object as output.
An input to a function is also called an \textit{argument} to the function;
the output is called the function's \textit{value} for that argument.
The inputs and outputs are usually restricted to a certain class of objects,
called the function's \emph{domain} and \emph{codomain}, respectively.
For example,
the addition function takes two numbers as input and returns a number.
The square function takes a single number and returns a number.
The inputs and outputs don't need to be numbers.
There is an ``area'' function that takes a country as input and returns its area in (say) square kilometres.
And there is a ``mother'' function that takes a person as input and returns their mother.

If a function has domain $X$ and codomain $Y$,
we say that it is a function \emph{from $X$ to $Y$}.
If all inputs and outputs of a function belong to a set $X$,
we say that it is a function \emph{on $X$}.
So the addition function and the square function are functions on the set of numbers,
while the area function is a function from the set of countries to the set of numbers.

Some functions are not defined for all objects in their domain.
The division function, for example, takes two numbers as input and returns a number,
but it is undefined if the second input is zero.
Such functions are called \emph{partial}.

Functions are often associated with a recipe or algorithm
for determining the output for a given input.
There are well-known algorithms for computing sums or squares.
But this isn't part of the modern concept of a function.
Any mapping from inputs to outputs is a function,
even if there is no recipe for determining the output.

Since functions are just mappings from inputs to outputs,
they are fully determined by their values for each input.
Consider, for example, the function $g$ on the natural numbers that
takes a number $x$ and as input and returns
$x^{2}$ if Goldbach's conjecture is true
and $0$ if Goldbach's conjecture is false.
Goldbach's conjecture says that
every even number greater than 2 is the sum of two primes.
It is not known whether the conjecture is true.
So we don't know what $g$ returns for inputs other than 0.
But we know that
$g$ is either identical to the square function or
to the constant function that returns 0 for every input.
Both of these are trivial to compute.
So we know that $g$ is trivial to compute,
even though we don't know its value for 1!

\begin{exercise}
  Give an example of a function with 3 arguments.
\end{exercise}

Let's now add function symbols to our first-order languages.
These combine with singular terms to form new singular terms.
For example,
if `$f$' is a two-place function symbol and `$a$' and `$b$' are individual constants,
then `$f(a,b)$' is a singular term;
it denotes the value of the function $f$ for the arguments $a$ and $b$.
`$f(f(a,b),c)$' is another singular term;
it denotes the value of $f$ for the arguments $f(a,b)$ and $c$.
Previously,
all singular terms were just individual constants and variables,
now they can be arbitrarily complex.
So we need a recursive definition.

\begin{definition}{}{terms}
  A \emph{(singular) term} of a first-order language $\L_{1}^{=}$ with functions and identity is a finite string conforming to the following formation rules.
  \begin{itemize}
    \item Every variable and every individual constant is a singular term.
    \item If $f$ is an $n$-ary function symbol ($n>0$) and $t_1,\ldots,t_n$ are singular terms then $f(t_1,\ldots,t_n)$ is a singular term.
  \end{itemize}
\end{definition}
%
A singular term is \emph{closed} if it contains no variables.
Formulas and sentences are defined exactly as before.

% By `$f(t_1,\ldots,t_n)$' I mean the string that begins with $f$, followed by the
% opening parenthesis, followed by the terms $t_1,\ldots,t_n$ separated by commas,
% and ending with a closing parenthesis.

% We could allow for zero-place function symbols. A (total) zero-ary function
% takes nothing as input and returns a single value; a zero-place function symbol
% $f$ simply denotes this value; it behaves just like a name.

Officially,
function symbols are placed in front of their arguments,
with parentheses and commas to separate the arguments.
By this convention,
`$(a \times b) + c$' is written `$+(\times(a,b),c)$'.
For the sake of readability,
we allow the more familiar infix notation as a metalinguistic ``abbreviation''.

% \begin{definition}{}{formulas=}
%   A \emph{formula} of $\L_{1}^{=}$ is a finite string built up according to the
%   following formation rules:
%   \begin{enumerate*}
%     \item[(i)] If $P$ is an $n$-ary predicate symbol and $t_{1},\ldots,t_{n}$ are singular terms then $Pt_{1}\ldots t_{n}$ is a formula.
%     \item[(ii)] If $A$ is a formula, then so is $\neg A$.
%     \item[(iii)] If $A$ and $B$ are formulas, then so is $(A \to B)$.
%     \item[(iv)] If $x$ is a variable and $A$ is a formula then $\forall x A$ is a formula.
%   \end{enumerate*}
% \end{definition}

\begin{exercise}
  Write down a first-order sentence expressing Lagrange's Theorem,
  that every natural number is the sum of four squares.
  Use a language with individual constants `0', `1', `2', `3', ...,
  and function symbols `+' and `$\times$' for addition and multiplication.
\end{exercise}

% \begin{exercise}
%   Suppose we used infix notation without parenthesis, writing, for example, x+y
%   for the sum of x and y. This would cause a problem with more complex terms
%   that contain functional terms. Can you explain the problem? [Think of
%   subtraction: x-y-z has two readings. We want \textit{unique readability}.]
% \end{exercise}

In the axiomatic calculus,
we generalize A4 to allow for closed terms $t$ where we previously had individual constants $c$:
%
\begin{axioms}
  A4 & $\forall x A \to A(x/t)$.
\end{axioms}
%
\noindent%
`$(x/t)$' is the obvious extension of the substitution notation to closed terms.

We don't need any new axioms or rules for function symbols.
But we introduce two new axiom schemas for identity:

\begin{axioms}
A6 & $t_1=t_1$\\
A7 & $t_1 = t_2 \to (A(x/t_1 )\to A(x/t_2))$
\end{axioms}
\noindent%
Here,
$t_1$ and $t_2$ are closed terms and $A$ is a formula in which only $x$ is free,
so that all instances of the schemas are closed.
A7 is often called \emph{Leibniz' Law}.
The idea is that if $t_1$ and $t_2$ are the very same object,
then anything true of $t_1$ is also true of $t_2$.

\begin{exercise}
  You may wonder why I didn't write A7 as ${t_1 = t_2} \to (A \to A(t_1/t_2))$.
  In response,
  explain why the following sentence is an instance of A7,
  but not of the alternative formulation:
  $a=b \to (Raa \to Rab)$.
\end{exercise}

\begin{exercise}\label{ex:identity}
  Show: \; (a) if $\Gamma \vdash t=s$ then $\Gamma \vdash s=t$,\;
  (b) if $\Gamma \vdash t=s$ and $\Gamma \vdash s=r$ then $\Gamma \vdash t=r$.
\end{exercise}

% \begin{exercise}
%   Show that if $\Gamma \vdash t=s$ then
%   $\Gamma \vdash f(t) = f(s)$.
% \end{exercise}

We also need to adjust our semantics.
The definition of a model remains the same as before,
except that interpretation functions need to interpret the function symbols.
We assume that all function symbols denote total functions on the model's domain.

\begin{definition}{}{model=}
  A \emph{model} $\Mod{M}$ of a first-order language $\L_{1}^{=}$ with functions and identity consists of
  \begin{cenumerate}
    \item[(i)] a non-empty set $D$ and
    \item[(ii)] a function $I$ that assigns
    \begin{itemize*}
      \item to each individual constant of $\L_{1}^{=}$ a member of $D$,
      \item to each $n$-ary function symbol of $\L_{1}^{=}$ an $n$-ary total function on $D$, and
      \item to each non-logical $n$-ary predicate of $\L_{1}^=$ a set of $n$-tuples from $D$.
    \end{itemize*}
  \end{cenumerate}
\end{definition}

As before,
we write $\llbracket s \rrbracket^{\Mod{M}}$ for the denotation of a non-logical symbol $s$ in a model $\Mod{M}$.
We extend this notation to all singular terms:

\begin{definition}{}{denotation}
    Let $\Mod{M}$ be a model of a first-order language $\L_{1}^{=}$ and $I$ the interpretation function of $\Mod{M}$.
    \begin{cenumerate}
      \item[(i)] For any individual constant $c$, $\llbracket c \rrbracket^{\Mod{M}} = I(c)$.
      \item[(ii)] If $f$ is an $n$-ary function symbol and $t_1,\ldots,t_n$ are singular terms then
        $\llbracket f(t_1,\ldots,t_n) \rrbracket^{\Mod{M}} = I(f)(\llbracket t_1 \rrbracket^{\Mod{M}},\ldots,\llbracket t_n \rrbracket^{\Mod{M}})$.
    \end{cenumerate}
\end{definition}

With this,
satisfaction, entailment, and validity are defined essentially as before.
I'll only give the definition of satisfaction:

\begin{definition}{}{satisfaction=}
  An $\L_{1}^{=}$-sentence $A$ is true in a model $\Mod{M}$ (for short, $\Mod{M} \satisfies A$)
  if one of the following conditions holds.
  \begin{enumerate*}
    \item[(i)] $A$ has the form $t_{1} = t_{2}$ and $\dn{t_{1}}^{\Mod{M}} = \dn{t_{2}}^{\Mod{M}}$.
    \item[(ii)] $A$ is any other atomic sentence $Pt_{1}\ldots t_{n}$ and
       $\t{\llbracket t_{1}\rrbracket^{\Mod{M}},\ldots,\llbracket t_{n}\rrbracket^{\Mod{M}}}$ is in $\llbracket P \rrbracket^{\Mod{M}}$.
    \item[(iii)] $A$ is of the form $\neg B$ and $\Mod{M} \not\satisfies B$.
    \item[(iv)] $A$ is of the form $(B \to C)$ and $\Mod{M} \not\satisfies B$ or $\Mod{M} \satisfies C$.
    \item[(v)] $A$ is of the form $\forall x B$ and $\Mod{M}' \satisfies B(x/c)$ for every
      model $\Mod{M}'$ that differs from $\Mod{\Mod{M}}$ at most in the object assigned to $c$,
      where $c$ is the alphabetically first individual constant that does not occur in $B$.
  \end{enumerate*}
\end{definition}

\begin{exercise}
  Explain why $\satisfies a=a$, if $a$ is an individual constant.
\end{exercise}

\begin{exercise}
  Define a model in which `$1+1=2$' is true and another in which it is false.
\end{exercise}

\begin{exercise}\label{ex:domain-size}
  Construct a sentence with `=' as the only predicate symbol that is true in a model $\Mod{M}$ iff the domain of $\Mod{M}$ has (a) at least two members, (b) at most two members, (c) exactly two members.
\end{exercise}

Two final comments.

One,
it would be nice if we could allow for partial functions and empty domains.
The problem is that
we would then have to deal with ``empty'' terms that don't pick out anything.
(On an empty domain,
every term is empty;
if $f$ denotes a partial function, $f(a)$ may be empty.)
If $t$ is empty,
should we say that $t=t$ be true?
What about its negation, $t\not=t$?
These questions can be answered in different ways,
leading to different versions of \emph{free logic}.
A free logic is simply a logic in which terms can be empty.

Two.
I've stipulated at the very start of this chapter that
a first-order language must have infinitely many individual constants.
This,
too,
is somewhat unsatisfactory.
Formalized theories of arithmetic or set theory or Newtonian mechanics,
for example,
typically don't involve infinitely many non-logical symbols.
Indeed,
the standard first-order theory of sets has only one non-logical symbol:
the membership predicate `$\in$'.

Why, then, did I require infinitely many individual constants?
There are two reasons.
The first arises in our (Mates-style) semantics of quantifiers:
Clause (v) in definition~\ref{def:satisfaction=}
interprets $\forall x A$ in terms of $A(x/c)$,
where $c$ is a constant that doesn't occur in $A$.
Because quantifiers can be nested without limit,
this requires an unending supply of constants:
we need two constants for the interpretation of $\forall x \forall y A$,
three for $\forall x \forall y \forall z A$,
and so on.
But these constants are only used in the internal semantic machinery.
Their original denotation in the model is irrelevant:
it plays no role in clause (v).
A similar point applies to the other reason why we need an unending supply of constants.
In our first-order calculus,
deriving $\forall x B(x)$ from $\forall x A(x)$
often requires instantiation $\forall x A(x)$ to $A(c)$,
deriving $B(c)$,
and then applying Gen.
Here we also need an unending supply of constants
to deal with nested quantifiers.
But here, too, the meaning of these constants is irrelevant:
they are used to denote ``arbitrary'' objects.

So we need a large supply of constants
to play certain internal roles in our proof system and semantics.
Conceptually,
these constants resemble variables:
they are sometimes called \emph{eigenvariables}.
We might have decided to classify them as logical.
In later chapters,
when I speak of the non-logical expressions of, say, formalized set theory,
I will usually ignore the eigenvariables.

\iffalse

% ** Mathematical structures

Many important mathematical structures consist of a set of objects together with some operations and relations on these objects,
and sometimes with a designated object playing a special role.
For example, we might construe the structure of natural numbers as consisting of the set $\mathbb{N}$ of numbers 0,1,2,\ldots, the designated object 0, the addition operation $+$, the multiplication operation $\times$, and the less-than relation $<$.
It is common to package all this into a list: $(\mathbb{N}, 0, +, \times, <)$.

When we talk about this structure in a first-order language,
we'll naturally use $\mathbb{N}$ as the intended domain of discourse,
we'll have an individual constant for 0,
function symbols intended to express $+$ and $\times$,
and a predicate symbol for $<$.
In effect, the structure $(\mathbb{N}, 0, +, \times, <)$ then provides a model for our language.
That's why textbooks on mathematical logic often identify models with mathematical structures.

Careful: when specifying the algebra, we don't use the first-order object
language. So '+' here is metalanguage. The object language may also have a '+'
symbol or a '0' symbol, but they are not treated as logical. So their meaning
isn't fixed. When we talk about logical entailment, we need to consider any
possible way of interpreting '0' and '+'.

E.g, D = \{ Paris, Rome, Canberra \}, +(x,y) = Paris, $\times$(x,y) = Rome, 0 = Canberra.

Note that, technically, we don't allow for languages with only one or two individual constants.
We need an unending supply of constants for two reasons.
One, the interpretation of $\forall x A$ appeals to $A(x/c)$,
where $c$ is a constant that doesn't occur in $A$.
The interpretation of $\forall x \forall y A$ therefore involves two constants, and so on.
But it doesn't matter what $\Mod{M}$ assigns to these further constants.
This is also true for the other reason why we need extra constants:
in our calculus,
we often need to reason from $\forall x A(x)$ to $A(c)$,
where $c$ is a new constant,
so that if we have derived $B(c)$ from $A(c)$,
we can later apply Gen to get $\forall x B(x)$.
Here, too, the extra constants only serve a Hilfsfunktion.
They aren't used to pick out a particular object.
(Such constants are historically called \emph{eigenvariables}.)



% ** Isomorphic structures

\begin{definition}{Isomorphic Structures}{isomorphic}
  Structures $\Mod{A}$ and $\Mod{B}$ are \emph{isomorphic} if there is a bijection $f$ from the domain of $\Mod{A}$ to the domain of $\Mod{B}$ such that xxx
\end{definition}

Clearly, truth at a structure is preserved under isomorphisms. A full proof is in BBJ pp.140ff. Might move to next chapter?

\begin{exercise}
Show that isomorphisms are equivalence relations
\end{exercise}

% ** Exercises

\fi

\section{Soundness}
\label{sec:soundness}

We have defined two consequence relations:
the proof-theoretic (syntactic) relation $\vdash$,
and the model-theoretic (semantic) relation $\entails$.
How are they related?
Can we show that $\Gamma\vdash A$ iff $\Gamma \entails A$?
We can.
The \emph{completeness} direction,
from $\Gamma \entails A$ to $\Gamma \vdash A$,
is hard and will be treated in the next chapter.
The \emph{soundness} direction,
from $\Gamma \vdash A$ to $\Gamma \entails A$,
is comparatively easy.
As in the propositional case,
we only have to verify that
all axioms are valid
and that the rules preserve validity.
There is nothing terribly exciting about this proof,
but let's go through it anyway.

We'll need the following lemmas.

\begin{lemma}{Coincidence Lemma}{coincidence}
  If two models $\Mod{M}$ and $\Mod{M}'$
  have the same domain and
  agree on the interpretation of all non-logical symbols in an $\mathcal L_1^=$-sentence $A$, then
  $\Mod{M}\satisfies A$ iff $\Mod{M}'\satisfies A$.
\end{lemma}
\begin{proof}
  \emph{Proof}.
  The proof is a simple induction on the complexity of $A$.
  The base case is guaranteed by clauses (i) and (ii) in definition~\ref{def:satisfaction=}.
  The inductive step for $\neg$ and $\to$ is trivial.
  Let's look at the case where $A$ has the form $\forall x B$.

  Assume $\Mod{M} \not\satisfies \forall x B$.
  By clause (v) of definition~\ref{def:satisfaction=},
  this means that
  $\Mod{M}'' \not\satisfies B(x/c)$ for some model $\Mod{M}''$
  that differs from $\Mod{M}$ at most in the object assigned to a constant $c$
  that does not occur in $B$.
  Let $\Mod{M}'''$ be like $\Mod{M}'$ except that
  $\llbracket c \rrbracket^{\Mod{M}'''} = \llbracket c \rrbracket^{\Mod{M}''}$.
  Since $\Mod{M}$ and $\Mod{M}'$ agree on all symbols in $B$,
  and $c$ is not in $B$,
  $\Mod{M}''$ and $\Mod{M}'''$ agree on all symbols in $B(x/c)$.
  So by induction hypothesis,
  $\Mod{M}''' \not\satisfies B(x/c)$.
  By clause (v) of definition~\ref{def:satisfaction=},
  this means that $\Mod{M}' \not\satisfies \forall x B$.

  We've shown that if $\Mod{M} \not\satisfies \forall x B$ then $\Mod{M}' \not\satisfies \forall x B$.
  The converse direction can be shown by an exactly parallel argument.
  So $\Mod{M} \satisfies \forall x B$ iff $\Mod{M}' \satisfies \forall x B$.
  \qed
\end{proof}

\begin{lemma}{Extensionality Lemma}{extensionality}
  If $\llbracket t_1 \rrbracket^{\Mod{M}} = \llbracket t_2 \rrbracket^{\Mod{M}}$
  then $\Mod{M} \satisfies A(x/t_1)$ iff $\Mod{M} \satisfies A(x/t_2)$.
\end{lemma}

\begin{proof}
  \emph{Proof}.
  The proof is by induction on complexity of $A$.
  As before,
  the base case is guaranteed by clauses (i) and (ii) in definition~\ref{def:satisfaction=},
  and the inductive step for $\neg$ and $\to$ is trivial.
  The case where $A$ has the form $\forall y B$ needs some work.

  Assume
  \begin{equation}\tag{1}
    \Mod{M} \not\satisfies \forall y B(x/t_1).
  \end{equation}
  We'll show that $\Mod{M} \not\satisfies \forall y B(x/t_2)$.
  By (1) and definition~\ref{def:satisfaction=},
  we have
  \begin{equation}\tag{2}
    \Mod{M}^c \not\satisfies B(x/t_1)(y/c),
  \end{equation}
  where $c$ is the alphabetically first constant that does not occur in $B(x/t_1)$
  and $\Mod{M}^c$ is a model that differs from $\Mod{M}$ at most in
  the interpretation of $c$.
  Let $d$ be a constant distinct from $c$ that does not occur in $B(x/t_1)$ or $B(x/t_2)$.
  let $\Mod{M}^{dc}$ be like $\Mod{M}^c$ except that
  $\llbracket d \rrbracket^{\Mod{M}^{dc}} = \llbracket c \rrbracket^{\Mod{M}^c}$.
  Since $\Mod{M}^{dc}$ and $\Mod{M}^c$ agree on all symbols in $B(x/t_1)(y/c)$,
  we have,
  by the coincidence lemma,
  \begin{equation}\tag{3}
    \Mod{M}^c \satisfies B(x/t_1)(y/c) \text{ iff }\Mod{M}^{dc} \satisfies B(x/t_1)(y/c).
  \end{equation}
  By induction hypothesis,
  \begin{equation}\tag{4}
    \Mod{M}^{dc} \satisfies B(x/t_1)(y/c) \text{ iff }\Mod{M}^{dc} \satisfies B(x/t_1)(y/d).
  \end{equation}
  Let $\Mod{M}^d$ be like $\Mod{M}^{dc}$ except that
  $\llbracket c \rrbracket^{\Mod{M}^d} = \llbracket c \rrbracket^{\Mod{M}}$.
  Since $c$ does not occur in $B(x/t_1)$ and is distinct from $d$,
  $\Mod{M}^d$ and $\Mod{M}^{dc}$ agree on all symbols in $B(x/t_1)(y/d)$.
  So by the coincidence lemma,
  \begin{equation}\tag{5}
    \Mod{M}^{dc} \satisfies B(x/t_1)(y/d) \text{ iff }\Mod{M}^{d} \satisfies B(x/t_1)(y/d)
  \end{equation}
  Since $\Mod{M}^d$ agrees with $\Mod{M}$ on the interpretation of $t_1$ and $t_2$,
  $\dn{t_1}^{\Mod{M}^d} = \dn{t_2}^{\Mod{M}^d}$.
  So by induction hypothesis,
  \begin{equation}\tag{6}
    \Mod{M}^d \satisfies B(x/t_1)(y/d) \text{ iff }\Mod{M}^d \satisfies B(x/t_2)(y/d).
  \end{equation}
  From (2)--(6), we get
  \begin{equation}\tag{7}
    \Mod{M}^d \not\satisfies B(x/t_2)(y/d).
  \end{equation}
  Now let $e$ be the alphabetically first constant that does not occur in $B(x/t_2)$.
  Assume first that $e$ is distinct from $d$.
  Let $\Mod{M}^{ed}$ be like $\Mod{M}^d$ except that
  $\dn{e}^{\Mod{M}^{ed}} = \dn{d}^{\Mod{M}^d}$.
  Since $e$ doesn't occur in $B(x/t_2)(y/d)$,
  $\Mod{M}^{ed}$ and $\Mod{M}^d$ agree on all symbols in $B(x/y_2)(y/d)$.
  So by the coincidence lemma,
  \begin{equation}\tag{8}
    \Mod{M}^d \satisfies B(x/t_2)(y/d) \text{ iff } \Mod{M}^{ed} \satisfies B(x/y_2)(y/d).
  \end{equation}
  By induction hypothesis,
  \begin{equation}\tag{9}
    \Mod{M}^{ed} \satisfies B(x/t_2)(y/d)\text{ iff }\Mod{M}^{ed} \satisfies B(x/t_2)(y/e).
  \end{equation}
  Finally,
  let $\Mod{M}^e$ be like $\Mod{M}$ except that
  $\dn{e}^{\Mod{M}^e} = \dn{e}^{\Mod{M}^{ed}}$.
  By the coincidence lemma,
  \begin{equation}\tag{10}
    \Mod{M}^{ed} \satisfies B(x/t_2)(y/e) \text{ iff }\Mod{M}^{e} \satisfies B(x/t_2)(y/e).
  \end{equation}
  From (7), (8), (9), and (10), we get
  \begin{equation}\tag{11}
    \Mod{M}^e \not\satisfies B(x/t_2)(y/e).
  \end{equation}
  We assumed that $e$ is distinct from $d$.
  If $e$ and $d$ are the same constant,
  we get (11) directly from (7).
  From (11) and definition~\ref{def:satisfaction=},
  we conclude that
  \begin{equation}\tag{12}
    \Mod{M} \not\satisfies \forall y B(x/t_2).
  \end{equation}

  We've shown that
  if $\Mod{M} \not\satisfies \forall y B(x/t_1)$ then
  $\Mod{M} \not\satisfies \forall y B(x/t_2)$.
  Swapping $t_1$ and $t_2$ throughout the argument,
  we can equally show that
  if $\Mod{M} \not\satisfies \forall y B(x/t_2)$ then
  $\Mod{M} \not\satisfies \forall y B(x/t_1)$.
  So $\Mod{M} \satisfies \forall y B(x/t_1)$ iff
  $\Mod{M} \satisfies \forall y B(x/t_2)$.
  \qed
\end{proof}

\begin{theorem}{Soundness of the first-order calculus}{soundness}
  If $\Gamma \vdash A$, then $\Gamma \entails A$.
\end{theorem}
\begin{proof}
  \emph{Proof.}
  We first show a special case:
  if $\vdash A$ then $\entails A$.

  Assume $\vdash A$.
  So there is a sequence $A_{1},\dots,A_{n}$ with $A_n = A$ such that
  each $A_{k}$ in the sequence is
  either an axiom or follows from previous sentences by MP or Gen.
  We show by strong induction on $k$ that $\entails A_{k}$.

  \textit{Case 1.} $A_{k}$ is an instance of A1--A3.
  Then $\entails A_{k}$ by exercise \ref{ex:axioms-valid}
  and the fact that
  the interpretation of `$\neg$' and `$\to$' in definition~\ref{def:satisfaction=} is the same as in propositional logic.

  \textit{Case 2.} $A_{k}$ is an instance of A4: $\forall x\,B \to B(x/t)$.
  Let $\Mod{M}$ be any model that satisfies $\forall x B$.
  By definition~\ref{def:satisfaction=},
  this means that $\Mod{M}' \satisfies B(x/c)$ for every model $\Mod{M}'$ that differs from $\Mod{M}$ at most in the object assigned to $c$,
  where $c$ does not occur in $B$.
  Let $\Mod{M}'$ be a model of this kind with $\llbracket c \rrbracket^{\Mod{M}'} = \llbracket t \rrbracket^{\Mod{M'}}$.
  Since $B(x/t)$ is obtained from $B(x/c)$ by substituting $t$ for $c$,
  it follows by the extensionality lemma that
  $\Mod{M}' \satisfies B(x/t)$.
  Finally,
  since $c$ does not occur in $B(x/t)$,
  $\Mod{M}'$ and $\Mod{M}$ agree on the interpretation of
  all symbols in $B(x/t)$.
  So $\Mod{M} \satisfies B(x/t)$ by the coincidence lemma.
  This shows that any model that satisfies $\forall x B$ also satisfies $B(x/t)$.
  Hence every model satisfies $\forall x B \to B(x/t)$.

  \textit{Case 3.} $A_{k}$ is an instance of A5: $\forall x (A\to B)\to(A\to\forall x\,B)$,
  where $x$ is not free in $A$.
  Let $\Mod{M}$ be any model that doesn't satisfy $A \to \forall x B$.
  By definition~\ref{def:satisfaction=},
  this means that
  $\Mod{M} \satisfies A$ and
  $\Mod{M}' \not\satisfies B(x/c)$ for some model $\Mod{M}'$
  that differs from $\Mod{M}$ at most in the object assigned to some constant $c$
  that does not occur in $B$.
  Let $d$ be the alphabetically first constant that does not occur in either $A$ or $B$,
  and let $\Mod{M}''$ be like $\Mod{M}'$ except that
  $\llbracket d \rrbracket^{\Mod{M}''} = \llbracket c \rrbracket^{\Mod{M}'}$.
  By the extensionality lemma,
  $\Mod{M}'' \not\satisfies B(x/d)$.
  By the coincidence lemma,
  $\Mod{M}'' \satisfies A$.
  So $\Mod{M}'' \not\satisfies A\to B(x/d)$.
  Since $x$ is not free in $A$,
  $A \to B(x/d)$ is $(A \to B)(x/d)$.
  So $\Mod{M}'' \not\satisfies (A \to B)(x/d)$.
  By definition~\ref{def:satisfaction=},
  this means that
  $\Mod{M} \not\satisfies \forall x (A\to B)$.
  Contraposing,
  we've shown that
  any model that satisfies $\forall x (A\to B)$ satisfies $A \to \forall x B$.
  So every model satisfies $\forall x (A\to B)\to(A\to\forall x B)$.

  \textit{Case 4.} $A_{k}$ is an instance of A6: $t_1=t_1$.
  Then $\entails A_{k}$ by clause (i) of definition~\ref{def:satisfaction=}.

  \textit{Case 5.} $A_{k}$ is an instance of A7: $t_1=t_2\to(A(x/t_1)\to A(x/t_2))$.
  Let $\Mod{M}$ be any model that satisfies $t_1=t_2$.
  Then $\llbracket t_1 \rrbracket^{\Mod{M}} = \llbracket t_2 \rrbracket^{\Mod{M}}$.
  By the extensionality lemma,
  $\Mod{M} \satisfies A(x/t_1)$ iff $\Mod{M} \satisfies A(x/t_2)$.
  So any model that satisfies $t_1=t_2$ and $A(x/t_1)$ also satisfies $A(x/t_2)$.
  So every model satisfies $t_1=t_2\to(A(x/t_1)\to A(x/t_2))$.

  \textit{Case 6.} $A_{k}$ is obtained by MP from earlier lines $A_{i}$ and $A_{i}\to A_{k}$.
  By induction hypothesis, $A_{i}$ and $A_{i}\to A_{k}$ are valid.
  So $A_{k}$ is valid by clause (iii) of definition~\ref{def:satisfaction=}.

  \textit{Case 7.} $A_{k}$ is obtained by Gen from an earlier line $A_i$.
  So $A_k$ has the form $\forall x A_i(c/x)$.
  Let $\Mod{M}$ be any model.
  By definition~\ref{def:satisfaction=},
  we need to show that
  $\Mod{M}' \satisfies A_i(c/x)(x/d)$ for every model $\Mod{M}'$ that differs from $\Mod{M}$ at most in the object assigned to $d$,
  where $d$ is the alphabetically first individual constant that does not occur in $A_i(c/x)$.
  Take any such $\Mod{M}'$ and $d$.
  Let $\Mod{M}''$ be like $\Mod{M}'$
  except that $\llbracket c \rrbracket^{\Mod{M}''} = \llbracket d \rrbracket^{\Mod{M}'}$.
  By induction hypothesis,
  every model satisfies $A_i$;
  so $\Mod{M}'' \satisfies A_i$.
  If $d$ is $c$ then $\Mod{M}'' = \Mod{M}'$.
  Otherwise $c$ does not occur in $A_i(c/x)(x/d)$.
  Either way,
  $\Mod{M}''$ and $\Mod{M}'$ agree on the interpretation of
  every symbol in $A_i(c/x)(x/d)$.
  By the coincidence lemma,
  it follows that
  $\Mod{M}' \satisfies A_i(c/x)(x/d)$.

  This completes the induction.
  We've shown that if $\vdash A$ then $\entails A$.
  Now assume $\Gamma \vdash A$.
  That is,
  there is a deduction $A$ from $\Gamma$.
  This deduction can involve only finitely many sentences $A_{1},\ldots,A_{n}$ from $\Gamma$.
  So we also have $A_{1},\ldots,A_{n} \vdash A$,
  By the deduction theorem, it follows that
  $\vdash A_{1} \to (\ldots (A_{n} \to A) \ldots)$.
  From what we've just shown,
  we can infer that
  $\entails A_{1} \to (\ldots (A_{n} \to A) \ldots)$.
  It is easy to see that
  $\Gamma \models A \to B$ iff $\Gamma,A \models B$.
  So we have $A_{1},\ldots,A_{n} \models A$
  and thereby $\Gamma \models A$.
  \qed
\end{proof}

% \begin{exercise}
% Can we still use soundness to prove consistency of the axiomatization? Yes.
% \end{exercise}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "logic3.tex"
%%% End:
