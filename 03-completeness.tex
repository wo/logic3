\chapter{Completeness}

\iffalse

Summary:

A1. A \to (B \to A)
A2. (A \to (B \to C)) \to ((A \to B) \to (A \to C))
A3. (\neg A \to \neg B) \to (B \to A)
A4. \forall x A \to A(x/t), where c is a closed term
A5. \forall x(A \to B) \to (A \to \forall x B), if x is not free in A
A6. t_1=t_1, for any closed term t_1
A7. t_1 = t_2 \to (A(x/t_1 )\to A(x/t_2))
MP. From A and A \to B one may infer B
Gen. From A one may infer \forall x A(t/x), where t is a closed term

(Only closed sentences may occur in a proof.)

An $\L_{1}^{=}$-sentence $A$ is true in a model $M$ (for short, $M \models A$)
if one of the following conditions holds.
\begin{enumerate}
\item $A$ is an identity sentence $t_{1} = t_{2}$ and $[t_{1}]^{M} = [t_{2}]^{M}$.
\item $A$ is any other atomic sentence $Pt_{1}\ldots t_{n}$ and
    $([t_{1}]^{M},\ldots,) \in \iota_{M}(P)$.
\item $A$ is of the form $\neg B$ and $M \not\models B$.
\item $A$ is of the form $(B \to C)$ and $M \not\models B$ or $M \models C$.
\item $A$ is of the form $\forall x B$ and $M' \models B(x/c)$ for every
    model $M'$ that differs from $M$ at most in the object assigned to $c$,
    where $c$ is the alphabetically first individual constant that does not occur in $B$.
\end{enumerate}

\fi

In this chapter,
we're going to meet some important results about the powers and deficiencies of first-order logic.
Our starting point is the completeness theorem,
which shows that there is a mechanical way to check for first-order entailment.
We'll see that this positive result is tightly connected to some negative results:
the compactness theorem and the Löwenheim-Skolem theorems.
These results concern the \emph{size} of models,
by which we mean the size of their domain.
To fully appreciate their implications,
I'll begin with some background about the sizes of sets,
which will be needed in later chapters anyway.

\section{Cardinalities}\label{sec:cardinalities}

$\{$ Athens $\}$ is a set with a single member:
the city Athens.
We say that $\{$ Athens $\}$ has size 1.
$\{$ Athens, Berlin $\}$ has size 2.
$\{$ Athens, Berlin, Cairo $\}$ has size 3.
And so on.
It seems straightforward.
But now consider the set $\mathbb{N}$ of natural numbers 0, 1, 2, 3, \ldots.
What is its size?

The official set-theoretic term for the size of a set is \textit{cardinality}.
So $\{$ Athens, Berlin, Cairo $\}$ has cardinality 3.
Finite sets have a finite cardinalities, which are natural numbers.
But infinite sets also have a cardinality.
These aren't natural numbers,
but numbers of a more general sort,
called \emph{cardinals}.
The infinite cardinals are also known as the \emph{alephs},
because they are conventionally written using the Hebrew letter `$\aleph$' (``aleph'').
For example,
the cardinality of $\mathbb{N}$ is called `$\aleph_0$'.
This is the smallest infinite cardinal.
The next one is $\aleph_1$,
followed by $\aleph_2$, and so on.

% You may be puzzled what sort of things the alephs are.
% I'll give an answer in the next chapter.
% But note that it is equally unclear what sort of thing the natural numbers are.
% The more important question is how the alephs behave.
How do we determine the cardinality of an infinite set?
The crucial idea goes back to Galileo and Hume,
although it was only fully developed by Georg Cantor in the 19th century.
Cantor,
following Galileo and Hume,
suggested that two sets have the same cardinality iff there is a one-to-one correspondence between there members.

To make this precise, we define the notion of a one-to-one correspondence or bijection.

\begin{definition}{}{bijection}
  A function $f$ from a set $A$ to a set $B$ is a \emph{bijection}
  if it satisfies the following two conditions.
  \begin{enumerate*}
    \item[(i)] For every $x, y \in A$, if $f(x) = f(y)$ then $x = y$. (Injectivity)
    \item[(ii)] For every $b \in B$ there is some $a \in A$ such that $f(a) = b$. (Surjectivity)
  \end{enumerate*}
\end{definition}

Intuitively, a bijection pairs up each element of $A$ with exactly one element of $B$,
and vice versa,
so that every element of either set gets a unique partner.
As a shorthand,
let's say that sets $A$ and $B$ are \emph{equinumerous} if there is a bijection from $A$ to $B$.
(In this case, there is always also a bijection from $B$ to $A$.)

The Galileo-Hume-Cantor principle now says that
\emph{two sets have the same cardinality iff they are equinumerous}.
Obviously,
no finite set of numbers is equinumerous with $\mathbb{N}$.
So $\mathbb{N}$ has an infinite cardinality.
Let's introduce `$\aleph_0$' as a name for this cardinality.
Using the Galileo-Hume-Cantor principle,
we can determine,
for any other set,
whether it also has cardinality $\aleph_0$.

Consider,
for example,
the set of odd numbers $1, 3, 5, 7, \ldots$.
The following function $f$ is a bijection from $\mathbb{N}$ to the set of odd numbers:
\[
  f(n) = 2n + 1.
\]
The function maps 0 to 1, 1 to 3, 2 to 5, and so on.
Every natural number is mapped to a unique odd number,
and no odd number is left unmapped.
So the set of odd numbers also has cardinality $\aleph_0$.

Galileo found this paradoxical:
how can there be as many odd numbers as natural numbers,
given that the odd numbers are a proper subset of the natural numbers?
Never mind,
said Cantor:
the resulting theory of cardinalities is consistent and mathematically fruitful,
even if it may initially seem strange.

Sets that are equinumerous with $\mathbb{N}$ are also called \emph{countably infinite} or \emph{denumerable}.
A set is \emph{countable} if it is either finite or countably infinite.
The label alludes to the fact that
a bijection between $\mathbb{N}$ and another set
effectively assigns a ``counter'' to each member of the set.
For example,
the above bijection between $\mathbb{N}$ and the odd numbers
can be written as follows:
\begin{align*}
  0. 1\\
  1. 3\\
  2. 5\\
  3. 7\\
  \vdots
\end{align*}

\begin{exercise}
  Show that (a) the set of even natural numbers and (b) the set of integers $\ldots, -2, -1, 0, 1, 2, \ldots$ are both countably infinite.
\end{exercise}

% The set of integers $\mathbb{Z} = \{\ldots, -2, -1, 0, 1, 2, \ldots\}$ is also countably infinite.
% We can establish a bijection between $\mathbb{N}$ and $\mathbb{Z}$ by mapping:
% \begin{align*}
% 0 &\mapsto 0 \\
% 1 &\mapsto 1 \\
% 2 &\mapsto -1 \\
% 3 &\mapsto 2 \\
% 4 &\mapsto -2 \\
% 5 &\mapsto 3 \\
% &\vdots
% \end{align*}
% In general, we map even numbers $2n$ to positive integers $n$, and odd numbers $2n+1$ to negative integers $-n$.

% As the example illustrates,
% we can conceptualize a mapping from $\mathbb{N}$ to some set $S$ as a \emph{list}
% of the members of $S$.
% If we allow lists with repetitions,
% we can say that a set is countable iff its members can be put on an infinite list,
% each item of which has as ``index'' a natural number.

Are there any \emph{uncountable} sets?
We might suspect that the set of \emph{pairs} of natural numbers is uncountable.
But not so.
Cantor's \emph{zig-zag method} shows that
there is a bijection between $\mathbb{N}$ and the set of pairs of natural numbers.
We begin by arranging all pairs of natural numbers in a two-dimensional grid.

\begin{center}
  \input{figures/zigzag.tex}
\end{center}

We then define a path through this grid that visits each cell exactly once.
The orange arrows indicate that path.
It effectively enumerates all pairs $(x, y)$,
starting with $(0,0)$,
followed by $(0,1)$, $(1,0)$, $(2,0)$, $(1,1)$, $(0,2)$, and so on.
The enumeration amounts to a lists of all the pairs.
We get a bijection to $\mathbb{N}$ by assigning to each pair
its position in the list (starting with position 0).
Thus $(0,0)$ is mapped to 0,
$(0,1)$ to 1,
$(1,0)$ to 2,
and so forth.

(We can find a formula for this bijection.
As we follow the arrow,
the pairs \emph{before} any given pair $(x,y)$ comprise
all the pairs on the left-to-top diagonals before $(x,y)$,
which have 1, 2, 3, etc. pairs,
plus the $y$ pairs on the diagonal containing $(x,y)$ itself.
The total number of pairs preceding $(x,y)$ is therefore
$(1 + 2 + \ldots + (x+y)) + y$.
In exercise \ref{ex:gauss-sum},
you showed that $1+2+ \ldots + (x+y) = (x + y)(x + y + 1)/2$.
So the position of any pair $(x,y)$ in the enumeration is
$(x+y)(x+y+1)/2 + y$.)

% Since each rational number can be expressed as a pair of natural numbers (the numerator and denominator),
% the zig-zag argument also shows that the set of rational numbers $\mathbb{Q}$ is countably infinite.

\begin{exercise}
  Show that the set of ordered triples of natural numbers is countably infinite.
  % Can zig zag the cube. But easier: zig zag a 2D grid in which the rows are the pairs $(x,y)$ of natural numbers and the columns are natural numbers. We know that the pairs can be numerated.
\end{exercise}

% Example: the set of formulas of $L_A$ is enumerable.

But it's true that not all sets are countable.
Cantor established this with another powerful technique:
\emph{diagonalization}.

\begin{theorem}{Cantor's Theorem}{cantor}
  The set of all sets of natural numbers not countable.
\end{theorem}

\begin{proof}
  Assume for reductio that the set of all sets of natural numbers is countable.
  This means that we can list them as $S_0, S_1, S_2, \ldots$.
  Now consider the set $D = \{ n \in \mathbb{N} : n \notin S_n \}$.
  This is a set of natural numbers,
  so it must be somewhere in the list.
  That is,
  there is some $S_k$ such that $D = S_k$.
  Now the number $k$ is either in $D$ or not.
  If $k$ is in $D$
  then by definition of $D$,
  $k$ is not in $S_k$.
  This is impossible, as $D = S_k$.
  If $k$ is not in $D$,
  then by definition of $D$,
  $k$ is in $S_k$.
  Again, this is impossible.
  \qed
\end{proof}

We can again picture this method with a two-dimensional grid.

\begin{center}
  \input{figures/diagonal.tex}
\end{center}

Each row represents a set of natural numbers.
Each column stands for a number.
A checkmark indicates that the number is in the set,
a cross that it is not.
(So $0$ is in $S_0$, $1$ is not in $S_0$, 2 is not in $S_{0}$, and so on.)
Cantor's method now looks at the diagonal, in bold.
It defines the new set $D$ by \emph{inverting} the diagonal,
swapping crosses and checkmarks.
In the picture,
the inverted diagonal would begin with \xmarkd \cmarkd \xmarkd \xmarkd \cmarkd \ldots,
meaning that $D$ does not contain $0$,
does contain 1,
does not contain 2 and 3,
does contain 4,
and so on.
The so-defined set $D$ is called the \emph{antidiagonal} of the grid.
It can't be anywhere in the list.
And so there can be no list of all sets of natural numbers.

Theorem \ref{thm:cantor} can be strengthened:

\begin{theorem}{Cantor's Theorem (general version)}{cantor}
  For any set $A$,
  the set of subsets of $A$ has strictly greater cardinality than $A$.
\end{theorem}

\begin{proof}
  \emph{Proof}.
  The proof proceeds essentially as before.
  Suppose for reductio that there is a bijection $f: A \to \mathcal{P}(A)$,
  where $\mathcal{P}(A)$ (called the \emph{power set} of $A$)
  is the set of all subsets of $A$.
  Define the antidiagonal set $D = \{ X \in A : X \notin f(X) \}$.
  Since $D$ is a subset of $A$,
  it is in $\mathcal{P}(A)$.
  But it can't be in an output of $f$.
  For suppose it is.
  That is,
  suppose $D = f(X)$ for some $X \in A$.
  Either $X$ is in $D$ or not.
  If $X$ is in $D$,
  then by definition of $D$,
  $X$ is not in $f(X)$.
  But $f(X) = D$,
  so this is impossible.
  If $X$ is not in $D$,
  then by definition of $D$,
  $X$ is in $f(X)$.
  Again, this is impossible.
  \qed
\end{proof}

% Applied to the natural numbers, Cantor's theorem shows that $\mathcal{P}(\mathbb{N})$
% -- the set of all subsets of natural numbers --
% is uncountably infinite.
% Its cardinality is denoted $2^{\aleph_0}$ or $\mathfrak{c}$ (the cardinality of the continuum).
% In fact, $\mathcal{P}(\mathbb{N})$ is equinumerous with the real numbers $\mathbb{R}$.

Cantor's theorem reveals a hierarchy of infinities.
Starting with $\mathbb{N}$,
we can produce larger and larger infinities by taking power sets:
$\mathcal{P}(\mathbb{N})$ is larger than $\mathbb{N}$,
$\mathcal{P}(\mathcal{P}(\mathbb{N}))$ is larger than $\mathcal{P}(\mathbb{N})$,
and so on,
without end.
There are infinitely many infinite cardinals.

% (We have just learned that ``infinitely many'' is not a precise answer.
% Are there \emph{countably many} infinities -- that is, $\aleph_{0}$?
% Or $\aleph_{1}$?
% Or what?
% According to modern set theory,
% the number of infinities is so large that
% it is not measured by any cardinality.)

\begin{exercise}
  Show that if $A$ and $B$ are countably infinite sets, then their union $A \cup B$ is countably infinite.
\end{exercise}

\begin{exercise}
  Show that the set of finite subsets of $\mathbb{N}$ is countably infinite.
\end{exercise}


\section{Planning the completeness proof}

In section \ref{sec:prop-completeness},
we showed that all valid sentences in propositional logic
are derivable from the axiom schemata A1--A3 by Modus Ponens.
This particular result is wasn't easy to foresee,
but it wasn't a surprise that
there is \emph{some} mechanical way of checking if
a sentence of propositional logic is valid.
Models of propositional logic are essentially finitary.
A model is a truth-value assignment to the sentence letters.
Since each sentence of propositional logic is composed of finitely many proposition letters,
and there are only finitely many ways of assigning truth-values to these sentence letters,
it is to be expected that
there is a finitary algorithm for deciding whether all
of these assignments make the sentence true.
% (or whether all of them make the sentence true
% if they make some other sentences true).

% Of course,
% it's not too surprising that the precise proof system I've introduced is sound and complete.
% The axioms and rules were chosen with an eye to completeness.
% This is unlike the situation in some nonclassical logics,
% like intuitionistic logic,
% were the proof-theoretic angle takes priority
% and models are defined so that
% the desired proof system comes out sound and complete.
% Hamkins pp.183ff.

In first-order logic,
the situation very different.
A first-order model consists of an arbitrary set $D$
together with an interpretation of the non-logical vocabulary,
where, for example, every subset of $D$ is a possible interpretation of any monadic predicate.
Even if $D$ itself is countable,
this means that there are usually uncountably many models with that domain.
And $D$ doesn't have to be countable.
It can have any cardinality whatsoever.
The space of first-order models is enormous.
Their number is vastly greater than $\aleph_0$.
There is, in fact, no aleph big enough to measure the number of first-order models.
It is therefore astonishing that
there is a finitary, mechanical method
-- a proof system --
by which one can determine precisely the sentences
that are true across the realm of all first-order models.

The completeness theorem for first-order logic is noteworthy for other reasons as well.
For example,
it supports the conjecture
that all intuitively valid mathematical arguments can be formalized
as deductions in the first-order calculus.
Let's take for granted, for the sake of the argument, that
every mathematical statement can be expressed in a first-order language.
Now suppose some mathematical argument \emph{can't} be replicated in the first-order calculus.
By the completeness theorem,
it follows that
there is a model in which the premises are true but the conclusion false.
This strongly suggests that the argument wasn't valid.
We'll meet further applications of the completeness theorem later.
First we need to prove it.
% BBJ 185

\iffalse

Also,
suppose I like the number 1, the number 2, the number 3, etc.
It follows model-theoretically that I like all numbers.
But proofs are finite.
So you can't prove the conclusion from the premises.
Yet we have completeness for FOL!

What about the numbers argument?
This argument isn't \textit{logically} valid.
It's \textit{arithmetically} valid, we might say:
valid given the standard meaning of the arithmetical terms.
Arithmetical entailment is not mirrored by any finite proof system!

Completeness entails that all (first-order) logical truths are provable.
Obviously, not all truths are logical truths.
So completeness doesn't mean that all truths are provable.

\fi

% \begin{exercise}
% Which, if any, of these statements are correct? (1) The soundness theorem implies that every sentence provable from the Peano axioms is a true sentence of arithmetic; (2) The completeness theorem implies that every true sentence of arithmetic is provable from the Peano axioms. [yes/no]
% \end{exercise}

The first completeness proof for first-order Logic was developed by Kurt Gödel in 1929.
We're going to follow the simpler strategy due to Henkin (1949)
that we've already used in chapter~\ref{ch:propcal}.
Let's recall the key steps.

We want to show that
whenever a sentence $A$ is entailed by a set of sentences $\Gamma$
(meaning that every model that makes all members of $\Gamma$ true also makes $A$ true),
then there is a deduction of $A$ from $\Gamma$.
For short:
if $\Gamma \models A$ then $\Gamma \vdash A$.
The proof is by contraposition.
We assume $\Gamma \not\vdash A$ and derive $\Gamma \not\models A$,
as follows.

\begin{cenumerate}
\item Assume $\Gamma \not\vdash A$.
\item Infer that $\Gamma \cup \{ \neg A \}$ is consistent.
\item Show that $\Gamma \cup \{ \neg A \}$ can be extended to a maximal consistent set $\Gamma^{+}$.
\item Construct a model $\mathcal{M}$ based on $\Gamma^{+}$ in which all members of $\Gamma^{+}$ are true.
\item Infer that $\Gamma \not\models A$.
\end{cenumerate}

% Why this fails for SOL: https://math.stackexchange.com/questions/4913675/how-completeness-fails-in-second-order-logic

A set of sentences is \emph{consistent}
if no contradiction can be derived from it.
That is,
$\Gamma$ is consistent iff
there is no sentence $A$ such that $\Gamma \vdash A$ and $\Gamma \vdash \neg A$.
Equivalently
(as you've shown in exercise \ref{ex:inconsistency}:
the proof carries over from propositional logic),
$\Gamma$ is consistent iff
$\Gamma \not\vdash \bot$.

Steps 2 and 5 are easy.
The following lemma establishes step 2,
and is proved exactly like its propositional counterpart,
lemma \ref{lem:consistency-lemma}.

\begin{lemma}{}{negation-consistency1}
  $\Gamma \not\vdash A$ iff $\Gamma \cup \{\neg A\}$ is consistent.
\end{lemma}
\begin{proof}
  \emph{Proof.}
  Suppose $\Gamma \cup \{\neg A\}$ is inconsistent.
  Then $\Gamma \vdash \neg \neg A$ by RAA
  and so $\Gamma \vdash A$ by DNE.
  Contraposing,
  this means that if $\Gamma \not\vdash A$ then $\Gamma \cup \{ \neg A \}$ is consistent.
  Conversely,
  suppose $\Gamma \vdash A$.
  Then $\Gamma, \neg A \vdash A$ by Mon
  and $\Gamma, \neg A \vdash \neg A$ by Mon and Id.
  So $\Gamma \cup \{ \neg A \}$ is inconsistent. \qed
\end{proof}

It remains to fill in steps 3 and 4:
we need to show that
\emph{every consistent set of sentences is satisfiable},
meaning that there is a model in which all its members are true.

Here is the plan.
Let $\Gamma$ be a consistent set of sentences in some first-order language $\L$
(with identity and function symbols).
We'll show how one can construct a model $\mathcal{M}_{\Gamma}$
in which all members of $\Gamma$ are true.

As in chapter \ref{ch:propcal},
it helps to extend $\Gamma$ to a maximal consistent set $\Gamma^{+}$.
For example,
if $\Gamma$ contains $Fa \lor Gb$,
it's not obvious if
we our model should satisfy $Fa$ or $Gb$ or both.
$\Gamma^{+}$ will contain $Fa$ or $Gb$ or both,
so it answer this question.
The atomic sentences in $\Gamma^{+}$ will instruct us how to build the model.

Suppose we have $Fa$ in $\Gamma^{+}$.
Our model $\mathcal{M}$ must then contain an object $\llbracket a\rrbracket^{\mathcal{M}}$ denoted by $a$ that falls in the extension $\llbracket F \rrbracket^{\mathcal{M}}$ of $F$.
The nature of the object $\llbracket a\rrbracket^{\mathcal{M}}$ is irrelevant.
For convenience,
we might choose \emph{the individual constant} $a$ as the object denoted by $a$.
Then we can say
that the extension of $F$ comprises all individual constants $c$ for which $Fc \in \Gamma^{+}$.
(It might seem odd to have a model whose domain consists of individual constants,
and in which each constant denotes itself.
But noting in the definition of a first-order model prevents us from doing this.)

Unfortunately,
there are some complications.
Suppose $\Gamma$ contains $\exists x Fx$.
(Which is short for $\neg \forall x \neg Fx$.)
We want this sentence to be true in our model $\mathcal{M}$.
So there must be some object in $\llbracket F \rrbracket^{\mathcal{M}}$.
But if $\llbracket F \rrbracket^{\mathcal{M}}$ is defined as the set of individual constants $c$ for which $Fc \in \Gamma^{+}$,
there might be no such object,
as there might be no constant $c$ for which $Fc \in \Gamma^{+}$.
We need to ensure that this doesn't happen.
That is,
we need to ensure that
for each existential sentence $\exists x Fx$ in $\Gamma^{+}$,
there is a ``witnessing'' sentence $Fc$ in $\Gamma^{+}$.
The problem is that
$\Gamma$ may already contain $\neg Fc$ for every individual constant $c$.
Then we can't add the required witness without making $\Gamma$ inconsistent.

To get around this problem,
we'll construct $\Gamma^{+}$ in an extend language $\L^{+}$
that adds new individual constants to $\L$.
We can then use these constants to ensure that
every existential sentence in $\Gamma$ has a witness.

Another complication arises from the presence of the identity symbol.
If we let each individual constant denote itself,
any identity statement involving different individual constants will be false.
After all,
no constant is identical to any other constant.
But $a=b$ is consistent,
and might be in $\Gamma$ and therefore in $\Gamma^{+}$.
So we'll actually let each constant denote a \emph{set} of terms:
the constant $a$ will denote the set $\{ c \mid a\!=\!c \in \Gamma^{+} \}$.
% We can't just use sets of terms,
% because we need to interpret f for all possible inputs,
% even if there's no constant c for which f(a)=c is in the Henkin set.

Let's fill in the details.

\section{The completeness proof}

We'll show that every consistent set of first-order sentences is satisfiable.
As we've seen above
(and as we'll spell out again below),
from this it is only a small step to the completeness theorem.

So let $\Gamma$ be a consistent set of sentences in a first-order language $\L$.
We'll extend $\Gamma$ to a maximal consistent set in a language $\L^{+}$
that has infinitely many further individual constants.
We first need to confirm that
$\Gamma$ is still consistent in the extended language $\L^{+}$.
Consistency is language-relative because
the language determines which instances of the axioms are available.
As we switch from $\L$ to $\L^{+}$,
new axioms become available.
We need to confirm that
these new axioms don't allow deriving a contradiction from $\Gamma$.

\begin{lemma}{}{consistency-in-Lplus}
  If $\Gamma$ is a set of $\L$-sentences that is consistent within $\L$,
  and $\L^{+}$ extends $\L$ by a set of new individual constants,
  then $\Gamma$ is consistent within $\L^{+}$.
\end{lemma}

\begin{proof}
  \emph{Proof}.
  Assume for contraposition that $\Gamma$ is inconsistent within $\L^{+}$.
  Then there is a deduction $A_{1},\ldots,A_{n}$ of $\bot$ from $\Gamma$,
  where each $A_{i}$ is an $\L^{+}$-sentence.
  Being finite,
  the deduction only uses finitely many of the new constants:
  call them $c_{1},\ldots,c_{k}$.
  The deduction also can't use more than finitely many of the old constants in $\L$.
  Since first-order languages have infinitely many constants,
  we can choose $k$ distinct constants $d_{1},\ldots,d_{k}$ from $\L$
  that don't occur in the deduction.
  Now consider the sequence of sentences $A_{1}',\ldots,A_{n}'$ that results from
  $A_{1},\ldots,A_{n}$ by replacing each $c_{i}$ by $d_{i}$.
  It is easy to see that
  \begin{enumerate*}
    \item[(i)] if $A_{i}$ is an axiom then so is $A_{i}'$;
    \item[(ii)] if $A_{i} \in \Gamma$ then $A_{i}' \in \Gamma$
      (sentences in $\Gamma$ don't contain any new constants);
    \item[(iii)] if $A_{i}$ follows from $A_{1},\ldots,A_{i-1}$ by MP or Gen,
      then $A_{i}'$ follows from $A_{1}',\ldots,A_{i-1}'$ by MP or Gen.
  \end{enumerate*}
  So $A_{1}',\ldots,A_{n}'$ is a deduction of $\bot$ from $\Gamma$,
  showing that $\Gamma$ is inconsistent within $\L$.
  \qed
\end{proof}

Next,
we show that
$\Gamma$ can be extended to a maximal consistent set $\Gamma^{+}$ in which
every existential sentence $\exists x A$ has a witness $A(x/c)$.
Such sets are called \emph{Henkin sets}.

\begin{definition}{}{henkin-set}
  A set of sentences $\Gamma$ in a first-order language $\L$ is a \emph{Henkin set in $\L$} if it satisfies the following conditions.
  \begin{enumerate*}
    \item[(i)] $\Gamma$ is consistent (within $\L$).
    \item[(ii)] For every $\L$-sentence $A$, either $A \in \Gamma$ or $\neg A \in \Gamma$. (Maximality)
    \item[(iii)] Whenever $\Gamma$ contains a sentence of the form $\neg \forall x A$
    then it also contains $\neg A(x/c)$ for some individual constant $c$. (Witnessing)
  \end{enumerate*}
\end{definition}
%
In the presence of (i) and (ii),
(iii) is equivalent to the requirement that
if $\Gamma$ contains $\neg \forall x \neg A$
then it contains $A(x/c)$ for some $c$.

\begin{exercise}\label{ex:henkin-closure}
  Show that if $\Gamma$ is a Henkin set and $\Gamma \vdash A$, then $A \in \Gamma$.
  % Proof exactly as for lem:max-cons-closure.
\end{exercise}

\begin{exercise}\label{ex:henkin-terms-constants}
  Show that if $\Gamma$ is a Henkin set (in first-order language with identity) then
  for every closed term $t$ of the language
  there is an individual constant $c$ such that $t=c$ is in $\Gamma$.
  % t=t entails ∃x(x=t). Witnessing requires x=c.
\end{exercise}

\begin{exercise}\label{ex:henkin-quantifiers}
  Show that if $\Gamma$ is a Henkin set then
  $\forall x A \in \Gamma$ iff $A(x/c) \in \Gamma$ for every individual constant $c$.
\end{exercise}

\begin{lemma}{}{henkin-extension}
  Every consistent set of $\L$-sentences $\Gamma$ can be extended
  to a Henkin set $\Gamma^+$ in any language $\L^{+}$ that adds infinitely many individual constants to $\L$.
\end{lemma}

\begin{proof}
  Let $\Gamma$ be a consistent set of $\L$-sentences.
  Let $A_{1}, A_{2}, \ldots$ be a list of all $\L^{+}$-formulas with exactly one free variable.
  We define a sequence of sets $\Gamma_{0}, \Gamma_{1}, \ldots$ as follows:
  \begin{align*}
    \Gamma_{0} & := \Gamma \\
    \Gamma_{n+1} & := \Gamma_{n} \cup \{ \neg \forall x A_{n} \to \neg A_{n}(x/c_{n}) \},
  \end{align*}
  where $x$ is the free variable in $A_{n}$ and
  $c_{n}$ is a new $\L^{+}$-constant that does not occur in $\Gamma_{n}$.
  (There must be some such constant because $\L^{+}$ contains infinitely many constants
  that don't occur in $\Gamma$.)
  Let $\Gamma'$ be the union $\bigcup_{n} \Gamma_{n}$ of all sets in this sequence.
  (That is, a sentence $A$ is in $\Gamma'$ iff it is in some $\Gamma_{n}$.)

  % Careful: it's not enough to add witnesses for all existential sentences,
  % or to loop only over the universal sentences in $\L$.
  % E.g., if $\Gamma$ contains ∃x∃yRxy, we'd only get the witness ∃yRay.
  % We also want a witness for that, which isn't in Γ and not even in \L, due to the new constant a.

  We show that $\Gamma'$ is consistent.
  Suppose not.
  Then there is a derivation of $\bot$ from $\Gamma$
  and the ``Henkin sentences'' $\neg \forall x A_{n} \to \neg A_{n}(x/c_{n})$.
  This derivation can use only finitely many of the Henkin sentences.
  So one of the $\Gamma_{n}$ must be inconsistent.
  But we can show by induction on $n$ that each $\Gamma_{n}$ is consistent.

  The base case, for $n=0$, hold by assumption: $\Gamma$ is consistent.

  For the inductive step, assume $\Gamma_{n}$ is consistent and
  suppose for reductio that $\Gamma_{n+1}$ is inconsistent.
  By RAA, we then have
  \begin{gather}
    \Gamma_{n} \proves \neg (\neg\forall x A_{n} \to \neg A_{n}(x/c_{n})). \tag{1}
  \end{gather}
  It's easy to show that
  if $\Gamma_n \proves \neg (A \to B)$ then
  $\Gamma_n \proves A$ and $\Gamma_n \proves B$.
  (Both $\neg (A \to B) \to A$ and $\neg (A \to B) \to B$ are truth-functional tautologies.)
  So (1) implies
  \begin{gather}
    \Gamma_{n} \proves \neg \forall x A_{n}, \text{ and} \tag{2}\\
    \Gamma_{n} \proves \neg\neg A_{n}(x/c_{n}). \tag{3}
  \end{gather}
  From (3), we get $\Gamma_{n} \proves A_{n}(x/c_{n})$ by DNE.
  As $c_{n}$ does not occur in $\Gamma_{n}$,
  Gen yields
  \begin{gather}
    \Gamma_{n} \proves \forall x A_{n}. \tag{4}
  \end{gather}
  (2) and (4) show that $\Gamma_{n}$ is inconsistent,
  which contradictions the induction hypothesis.

  Next, we extend $\Gamma'$ to a maximal consistent set $\Gamma^{+}$.
  The construction follows the proof of Lindenbaum's Lemma (lemma \ref{lem:lindenbaum}).
  Let $S_{1}, S_{2}, \ldots$ be a list of all $\L^{+}$-sentences.
  Starting with $\Gamma'$,
  We define another sequence of sets $\Gamma'_{0}, \Gamma'_{1}, \ldots$:
  \begin{align*}
    \Gamma'_{0} & := \Gamma' \\
    \Gamma'_{n+1} & := \begin{cases}
        \Gamma'_{n} \cup \{ S_{n} \} & \text{if } \Gamma'_{n} \cup \{ S_{n} \} \text{ is consistent,} \\
        \Gamma'_{n} \cup \{ \neg S_{n} \} & \text{otherwise.}
        \end{cases}
  \end{align*}

  We show by induction that each $\Gamma'_{n}$ is consistent.
  The \emph{base case}, for $n=0$, holds because $\Gamma'_0$ is $\Gamma'$,
  which we've just shown to be consistent.
  For the \emph{inductive step},
  assume that a set $\Gamma'_n$ in the list is consistent.
  By lemma \ref{lem:extension-consistency1},
  either $\Gamma'_n \cup \{ S_n \}$ or $\Gamma'_n \cup \{ \neg S_n \}$ is consistent.
  If $\Gamma'_n \cup \{ S_n \}$ is consistent,
  then $\Gamma'_{n+1}$ is $\Gamma'_n \cup \{ S_n \}$ (by construction),
  so $\Gamma'_{n+1}$ is consistent.
  If $\Gamma'_n \cup \{ S_n \}$ is not consistent,
  then $\Gamma'_{n+1}$ is $\Gamma'_n \cup \{ \neg S_n \}$,
  so again $\Gamma'_{n+1}$ is consistent.

  Let $\Gamma^{+}$ be the union $\bigcup_{n} \Gamma'_{n}$ of $\Gamma'_{0}, \Gamma'_{1}, \ldots$.
  Evidently, $\Gamma$ is a subset of $\Gamma^{+}$.
  We show that $\Gamma^{+}$ is a Henkin set.

  Maximality holds because for each sentence $S_n$,
  one of $S_n$ or $\neg S_n$ is in $\Gamma'_{n+1}$ and therefore in $\Gamma^{+}$.

  To show consistency,
  suppose that $\Gamma^{+}$ is inconsistent.
  Then there are sentences $A_1,\ldots,A_n$ in $\Gamma^+$ from which $\bot$ is deducible.
  All of these sentences have to occur somewhere on the list $S_1,S_2,\ldots$.
  Let $S_j$ be the first sentence from $S_1,S_2,\ldots$ that occurs after
  all the $A_1,\ldots,A_n$.
  Then all $A_1,\ldots,A_n$ are  in $\Gamma'_j$.
  So $\Gamma'_j$ is inconsistent.
  But we've seen that all of the $\Gamma'_n$ are consistent.

  It remains to show that $\Gamma^{+}$ has the witnessing property:
  whenever $\neg \forall x A$ is in $\Gamma^{+}$ then
  $\Gamma^+$ contains a corresponding sentence $\neg A(x/c)$.
  Let $A$ be any formula in which $x$ is the only free variable.
  By construction,
  $\Gamma'$ contains $\neg \forall x A \to \neg A(x/c)$,
  for some constant $c$.
  Since $\Gamma^{+}$ extends $\Gamma'$,
  it also contains this sentence.
  So if $\Gamma^{+}$ contains $\neg \forall x A$
  then it contains $\neg A(x/c)$, by lemma~\ref{lem:max-cons-closure1}.
  \qed
\end{proof}

Next,
we show how to read off a model from a Henkin set.
The model's domain will consist of sets of closed terms,
so that we can stipulate that
each constant $c$ denotes the set of all terms $t$ for which $c=t$ is in the Henkin set.
Let's have a closer look at these sets.

\begin{definition}{}{equivalence}
  A binary relation $R$ on some domain $D$ is an \emph{equivalence relation} if it is
  \begin{cenumerate}
    \item[(i)] \emph{reflexive}: for every $x \in D$, $x R x$;
    \item[(ii)] \emph{symmetric}: for every $x, y \in D$, if $x R y$ then $y R x$; and
    \item[(iii)] \emph{transitive}: for every $x, y, z \in D$, if $x R y$ and $y R z$ then $x R z$.
  \end{cenumerate}
\end{definition}

\begin{lemma}{}{=equivalence}
  If $\Gamma$ is a Henkin set then
  the relation $R$ that holds between $\L^{+}$-terms $t,s$ iff $t=s \in \Gamma$ is an equivalence relation.
\end{lemma}
\begin{proof}
  \emph{Proof.}
  By A6, $\proves t=t$.
  So $t=t \in \Gamma$ by exercise~\ref{ex:henkin-closure}.
  Symmetry and transitivity follow similarly from exercise~\ref{ex:identity}.
  \qed
\end{proof}

\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
  \scalebox{0.7}{
  \input{figures/partition.tex}
  }
\end{wrapfigure}

An equivalence relation \emph{partitions} the domain over which it is defined into distinct cells so that
within each cell,
all objects stand in the relation to one another.
These cells are called \emph{equivalence classes}.
If $R$ is an equivalence relation and $x$ an object in the domain,
we write `$[x]_{R}$' for the equivalence class of $R$ that contains $x$.
That is, $[x]_{R} = \{ y \in D \mid x R y \}$.
If the relation $R$ is clear from the context,
we may simply write `$[x]$'.

When we're talking about a Henkin set $\Gamma$,
the relevant equivalence relation is the one defined in lemma \ref{lem:=equivalence}.
So `$[t]$' denotes the set of all terms $s$ for which $t=s \in \Gamma$.

If $\Gamma$ is a Henkin set,
the model $\mathcal{M}$ that we'll construct from $\Gamma$ will have as its domain
the set of all equivalence classes $[t]$,
where $t$ is a closed term in the language of $\Gamma$.
By exercise~\ref{ex:henkin-terms-constants},
$[t]$ always contains an individual constant.
So we can also say that $D = \{ [c] \mid c \text{is an individual constant} \}$.

We'll stipulate that
the interpretation function $I$ of $\mathcal{M}$
assigns $[c]$ to each individual constant $c$.
So we'll have $\llbracket c \rrbracket^{\mathcal{M}} = [c]$.
We'll extend this to function terms,
so that $\llbracket f(c) \rrbracket^{\mathcal{M}} = [f(c)]$.
But we can't \emph{directly} stipulate this,
because interpretation functions don't assign denotations to complex terms.
We have to interpret $f$ as denoting a function on $D$.
The denotation of $f(c)$ is then
the denotation of $f$ applied to the denotation of $c$.
Since the denotation of $c$ is $[c]$,
we want the denotation of $f$ to be a function that returns $[f(c)]$ for input $[c]$.
So we'll stipulate that for any one-place function symbol $f$ and constant $c$,
$I(f)$ is the function that maps $[c]$ to $[f(c)]$.

This kind of stipulation can go wrong.
Suppose $[c]$ contains two constants $c$ and $d$,
and $[f(c)] \neq [f(d)]$.
Our stipulation entails that $I(f)$
returns $[f(c)]$ for input $[c]$
and $[f(d)]$ for input $[d]$.
But a function can't return two different values for the same input.
we must show that this problem can never arise.

A similar issue arises for the interpretation of predicates.
To ensure that $Ft$ is in $\Gamma$ iff $\mathcal{M} \satisfies Ft$,
we'll stipulate that
$I(F)$ is the set of all $[t]$ for which $Ft \in \Gamma$.
This set isn't well-defined if
there are cases where $[t]$ contains two terms $t$ and $s$
for which $Ft \in \Gamma$ but $Fs \notin \Gamma$.

The following lemma shows that neither problem can arise.

\begin{lemma}{}{congruence}
  If $f$ is an $n$-ary function symbol,
  $P$ an $n$-ary predicate symbol,
  and $s_{1}, \ldots, s_{n}, t_{1}, \ldots, t_{n}$ are terms such that
  $[s_{1}] = [t_{1}], \ldots, [s_{n}] = [t_{n}]$,
  then
  \begin{cenumerate}
    \item[(i)] $[f(s_1,\ldots,s_n)] = [f(t_{1},\ldots,t_{n})]$;
    \item[(ii)] $P(s_1,\ldots,s_n) \in \Gamma \quad \text{iff} \quad P(t_1,\ldots,t_n) \in \Gamma$.
  \end{cenumerate}
\end{lemma}

\begin{proof}
  \emph{Proof.}
  Assume $[s_{i}] = [t_{i}]$, for $i=1,2,\ldots,n$.
  So $\Gamma$ contains $s_{i}\!=\!t_{i}$, for each $i=1,2,\ldots,n$.

  (i).
  By A6 (and exercise~\ref{ex:henkin-closure})),
  $f(s_{1},\ldots,s_{n})=f(s_{1},\ldots,s_{n})$ is in $\Gamma$.
  By $n$ instances of A7 and MP,
  it follows that $f(s_{1},\ldots,s_{n})=f(t_{1},\ldots,t_{n})$ is in $\Gamma$,
  which entails that $[f(s_1,\ldots,s_n)] = [f(t_{1},\ldots,t_{n})]$.

  (ii). Assume $P(s_{1},\ldots,s_{n})$ is in $\Gamma$.
  By $n$ instances of A7 and MP,
  $P(t_{1},\ldots,t_{n})$ is in $\Gamma$ as well.
  The converse holds by the same reasoning.
  \qed
\end{proof}

Lemma \ref{lem:congruence} ensures that the following definition is legitimate.

\begin{definition}{}{henkin-model}
  Let $\Gamma$ be a Henkin set in a language $\L$.
  The \emph{Henkin model} $\mathcal{M}_{\Gamma}$ of $\Gamma$ is defined as follows.

  The domain $D$ of $\mathcal{M}_{\Gamma}$ is the set
  $\{ [c] \mid c \text{ is an individual constant of $\L$} \}$,
  where $[c]$ is the set of all closed $\L$-terms $t$ for which $c=t \in \Gamma$.

  The interpretation function $I$ of $\mathcal{M}_{\Gamma}$ maps
  \begin{cenumerate}
    \item[(i)] each individual constant $c$ to $[c]$,
    \item[(ii)] each $n$-ary function symbol $f$ to the function on $D$ that maps
         any $n$-tuple $[t_1],\ldots,[t_n]$ to $[f(t_1,\ldots,t_n)]$,
    \item[(iii)] each (non-logical) $n$-ary predicate symbol $P$ to
         the set of all $n$-tuples $([t_1],\ldots,[t_n])$ such that $P(t_1,\ldots,t_n) \in \Gamma$.
  \end{cenumerate}
\end{definition}

Now remember what we're trying to achieve.
We want to show that
every consistent set of sentences is satisfiable.
We've shown in lemma~\ref{lem:henkin-extension} that
every such set can be extended to a Henkin set.
Definition~\ref{def:henkin-model} tells us
how to construct a model from this Henkin set.
It remains to show that
all members of the Henkin set
(and therefore all members of the original set)
are true in this model.

We'll need the following two facts.

\begin{lemma}{}{henkin-terms}
  If $\mathcal{M}$ is a Henkin model and $t$ a closed term then
  $\llbracket t \rrbracket^{\mathcal{M}} = [t]$.
\end{lemma}
\begin{proof}
  \emph{Proof}.
  The proof is by induction on complexity of $t$.
  The base case is covered by clause (i) in definition \ref{def:henkin-model}.
  So let $t$ be $f(t_{1},\ldots,t_{n})$.
  Then
  \begin{align*}
    \llbracket f(t_{1},\ldots,t_{n})\rrbracket^{\mathcal{M}} &= \llbracket f\rrbracket^{\mathcal{M}}(\llbracket t_{1}\rrbracket^{\mathcal{M}},\ldots,\llbracket t_{n}\rrbracket^{\mathcal{M}}) &&\text{ by def.\ \ref{def:satisfaction=}}\\
                               &= \llbracket f\rrbracket^{\mathcal{M}}([t_{1}],\ldots,[t_{n}]) &&\text{ by ind.\ hyp.}\\
                               &= [f(t_{1},\ldots,t_{n})] &&\text{ by def.~\ref{def:henkin-model}.} \qed
  \end{align*}
\end{proof}

\begin{lemma}{}{henkin-quantifiers}
  If $\mathcal{M}$ is a Henkin model and $B$ a formula then
  $\mathcal{M} \satisfies \forall x B$ iff
  $\mathcal{M} \satisfies B(x/c)$ for all individual constants $c$.
\end{lemma}
\begin{proof}
  \emph{Proof.}
  We first show that
  if $\mathcal{M}^{d:[c]}$ is just like $\mathcal{M}$
  except that it assigns $[c]$ to $d$,
  and $d$ does not occur in $B$,
  then
  \begin{equation}\tag{1}
    \mathcal{M}^{d:[c]} \satisfies B(x/d)\text{ iff }\mathcal{M} \satisfies B(x/c).
  \end{equation}
  There are two cases to consider.
  If $d$ is $c$ then $\mathcal{M}^{d:[c]}$ is $\mathcal{M}$ and (1) holds trivially.
  If $d$ is a constant other than $c$, then $d$ does not occur in $B(x/c)$.
  So $\mathcal{M}^{d:[c]}$ and $\mathcal{M}$ agree on the interpretation of all symbols in $B(x/c)$,
  and so (1) holds by lemma~\ref{lem:coincidence}.

  Now, by definition \ref{def:satisfaction=},
  $\mathcal{M} \satisfies \forall x B$ iff
  $\mathcal{M}' \satisfies B(x/d)$ for every model $\mathcal{M}'$
  that differs from $\mathcal{M}$ at most in the object assigned to $d$,
  where $d$ is the alphabetically first constant that does not occur in $B$.
  Since each such $\mathcal{M}'$ has the same domain as $\mathcal{M}$,
  the set of $\mathcal{M}'$ is precisely the set of
  variants $\mathcal{M}^{d:[c]}$ of $\mathcal{M}$,
  So $\mathcal{M} \satisfies \forall x B$ iff
  $\mathcal{M}^{d:[c]} \satisfies B(x/d)$ for every constant $c$,
  which, by (1), is the case iff
  $\mathcal{M} \satisfies B(x/c)$ for every constant $c$.
  \qed
\end{proof}

\begin{lemma}{Truth Lemma}{truth}
  If $\mathcal{M}$ is the Henkin model of a Henkin set $\Gamma$ in a language $\L$,
  then
  $\mathcal{M} \models A$ iff $A \in \Gamma$,
  for every $\L$-sentence $A$.
\end{lemma}

\begin{proof}
  By induction on  complexity of $A$.

  \emph{Base case}:
  $A$ is atomic. There are two subcases.

  Assume that $A$ is an identity sentence $s=t$.
  Then $\mathcal{M} \satisfies s=t$
  iff $\llbracket s\rrbracket^{\mathcal{M}} = \llbracket t\rrbracket^{\mathcal{M}}$
  by definition \ref{def:satisfaction=},
  iff $[s] = [t]$
  by lemma~\ref{lem:terms},
  iff $s=t \in \Gamma$,
  by definition of the equivalence classes.

  Assume next that $A$ is an atomic sentence $P(t_{1},\ldots,t_{n})$,
  where $P$ is non-logical.
  Then $\mathcal{M} \satisfies P(t_{1},\ldots,t_{n})$
  iff $(\llbracket t_{1}\rrbracket^{\mathcal{M}},\ldots,\llbracket t_{n}\rrbracket^{\mathcal{M}}) \in \llbracket P\rrbracket^{\mathcal{M}}$
  by definition \ref{def:satisfaction=},
  iff $([t_{1}],\ldots,[t_{n}]) \in \llbracket P\rrbracket^{\mathcal{M}}$
  by lemma~\ref{lem:terms},
  iff $P(t_{1},\ldots,t_{n}) \in \Gamma$
  by definition \ref{def:henkin-model}.

  \emph{Inductive step:}
  $A$ is composed of other sentences.
  We have three subcases.
  The first two,
  where $A$ is $\neg B$ or $B \to C$,
  are easy and work exactly as in lemma~\ref{lem:truth-lemma}.
  I'll skip them.

  % Assume that $A$ is a $\neg B$,
  % for some sentence $B$.
  % We need to show that $\mathcal{M} \satisfies \neg B$ iff $\neg B \in \Gamma$.
  % Left to Right:
  % Assume $\mathcal{M} \satisfies \neg B$.
  % Then $\mathcal{M} \not\satisfies B$ by definition \ref{def:satisfaction=}.
  % So $B \notin \Gamma$ by induction hypothesis.
  % So $\neg B \in \Gamma$ by maximality of $\Gamma$.
  % Right to Left:
  % Assume $\neg B \in \Gamma$.
  % Then $B \not\in \Gamma$ by consistency of $\Gamma$.
  % So $\mathcal{M} \not\satisfies B$ by induction hypothesis.

  % Assume next that $A$ is $B \to C$,
  % for some sentences $B$ and $C$.
  % We need to show that $\mathcal{M} \satisfies B \to C$ iff $B \to C \in \Gamma$.
  % Left to Right:
  % Assume $\mathcal{M} \satisfies B \to C$.
  % Then $\mathcal{M} \not\satisfies B$ or $\mathcal{M} \satisfies C$ by definition \ref{def:satisfaction=}.
  % If $\mathcal{M} \not\satisfies B$, then by induction hypothesis $B \notin \Gamma$,
  % so by maximality $\neg B \in \Gamma$.
  % Since $\neg B \proves B \to C$,
  % it follows that $B \to C \in \Gamma$.
  % If $\mathcal{M} \satisfies C$, then by induction hypothesis $C \in \Gamma$.
  % Since $C \proves B \to C$,
  % it again follows that $B \to C \in \Gamma$.
  % Right to Left:
  % Assume $B \to C \in \Gamma$.
  % If $B \in \Gamma$, then $C \in \Gamma$ by MP,
  % and by induction hypothesis $\mathcal{M} \satisfies C$.
  % By definition \ref{def:satisfaction=}, this means that
  % $\mathcal{M} \satisfies B \to C$.
  % If, on the other hand, $B \not\in \Gamma$
  % then by induction hypothesis $\mathcal{M} \not\satisfies B$,
  % and $\mathcal{M} \satisfies B \to C$ by definition \ref{def:satisfaction=}.

  For the final subcase,
  assume that $A$ is $\forall x B$,
  for some formula $B$.
  We have
  $\mathcal{M} \satisfies \forall x B$ iff
  $\mathcal{M} \satisfies B(x/c)$ for every constant $c$ by lemma~\ref{lem:henkin-quantifiers},
  iff $B(x/c) \in \Gamma$ for every constant $c$ by induction hypothesis,
  iff $\forall x B \in \Gamma$ by exercise~\ref{ex:henkin-quantifiers}.
  \qed
\end{proof}

With that,
all the ingredients for the completeness proof are in place.

\begin{theorem}{Completeness of the first-order calculus (Gödel 1929)}{completeness}
  If $\Gamma \models A$ then $\Gamma \proves A$.
\end{theorem}

\begin{proof}
  \emph{Proof.}
  We argue by contraposition.
  Assume $\Gamma \notproves A$.
  Then $\Gamma \cup \{\neg A\}$ is consistent,
  by lemma~\ref{lem:negation-consistncy1}.
  By lemma~\ref{lem:henkin-extension},
  $\Gamma \cup \{ \neg A \}$ can be extended to a Henkin set $\Gamma^{+}$ in an extended language $\L^{+}$.
  By the Truth Lemma,
  the Henkin model $\mathcal{M}_{\Gamma^{+}}$ of $\Gamma^{+}$ satisfies every sentence in $\Gamma^{+}$,
  and therefore every sentence in $\Gamma \cup \{\neg A\}$.
  So there is a model that satisfies all sets in $\Gamma$ but doesn't satisfy $A$.
  So $\Gamma \not\entails A$. \qed
\end{proof}

Technically,
the model $\mathcal{M}_{\Gamma^{+}}$ that figures in this proof is not
an model for the original language $\L$ of $\Gamma$ and $A$,
because it also interprets the added individual constants.
We can get an $\L$-model that falsifies $\Gamma \entails A$
by restricting the interpretation function of $\mathcal{M}_{\Gamma^{+}}$
to $\L$-constants,
without changing the domain.
(This is called the \emph{reduct} of $\mathcal{M}_{\Gamma^+}$ to $\L$.)

\begin{exercise}
  Assume that $\Gamma_{0}, \Gamma_{1}, \ldots$ are consistent sets of sentences
  in a first-order language $\L$,
  and that each $\Gamma_{i}$ is a subset of $\Gamma_{i+1}$.
  Show that their union $\bigcup_{i} \Gamma_{i}$ is consistent.
\end{exercise}

\section{Unintended Models}

We've shown that the first-order calculus is sound (theorem~\ref{thm:soundness})
and complete (theorem~\ref{thm:completeness}:
$\Gamma \entails A$ iff $\Gamma \proves A$.
As Gödel pointed out,
this has a curious consequence:

\begin{theorem}{Compactness of first-order logic (Gödel 1929)}{compactness}
   If every finite subset of a set is satisfiable then the set itself is satisfiable.
\end{theorem}

% The word 'compact', in this use, is inherited from topology,
\begin{proof}
  \emph{Proof.}
  Let $\Gamma$ be a set of first-order sentences.
  Assume for contraposition that $\Gamma$ is not satisfiable.
  So $\Gamma \entails \bot$.
  By completeness,
  it follows that $\Gamma \proves \bot$:
  there is a deduction of $\bot$ from $\Gamma$.
  This deduction can only use finitely many sentences from $\Gamma$.
  So there is a finite subset $\Gamma_{0}$ of $\Gamma$ for which $\Gamma_{0} \proves \bot$.
  By soundness,
  $\Gamma_{0} \entails \bot$.
\end{proof}

% \begin{corollary}{}{}
%   If $\Gamma \entails A$ then
%   there is a finite subset $\Gamma_{0}$ of $\Gamma$ such that $\Gamma_{0} \entails A$.
% \end{corollary}

Why is this curious?
Well, for one,
it is easy to come up with cases where a conclusion is entailed by infinitely many premises,
but not by any finite subset of those premises.
For example.
Consider the premises
`I like the number 0',
`I like the number 1',
`I like the number 2',
and so on, for all natural numbers.
Together,
these premises entail `I like all natural numbers'.
But no finite subset of the premises does.
This doesn't contradict the compactness theorem because the inference isn't \emph{logically} valid:
it depends on the interpretation of `0', `1', `2', \ldots, and `natural number'.
Compactness therefore implies that
there is no way to fully pin down the interpretation of these terms in first-order logic.

Let's think about this more carefully.
Many branches of mathematics can be seen as studying certain \emph{mathematical structures}.
A mathematical structure consists of a set of objects together with some operations and relations on these objects.
For example,
arithmetic studies operations and relations on the
natural numbers.
A formalized, first-order theory of arithmetic will therefore have
nonlogical symbols for, say, addition (`$+$'),
multiplication (`$\times$'),
and the less-than relation (`$<$').
We might add an individual constant `$n$' for each number $n$,
but for the sake of economy we can instead have a single constant `0' for 0
and another symbol `$s$' for the successor function
that maps each number $n$ to its successor $n+1$.
Instead of `1', we can then write `$s(0)$';
`2' is `$s(s(0))$', and so on.
In \emph{logic}, we abstract away from the meaning of the nonlogical symbols.
In \emph{arithmetic}, we obviously don't.
Here, `1+2=3'
(that is, `$s(0) + s(s(0)) = s(s(s(0)))$')
is a definite claim about the natural numbers.
We say that it has an \emph{intended interpretation},
or an \emph{intended model}.

The intended model of arithmetic, $\mathcal{N}$, has as its domain
the set of natural numbers $\mathbb{N}$;
it interprets `0' as denoting 0,
`+' as denoting the addition function +,
`$\times$' as denoting the multiplication function $\times$,
`$s$' as denoting the successor function $s$, and
`$<$' as denoting the less-than relation $<$.
We can package all these ingredients into a list:
$(\mathbb{N}, 0, +, \times, s, <)$.
This list represents the \emph{structure} of the natural numbers.
It identifies a set $\mathbb{N}$
and some operations and relations on that set.
(0 counts as a zero-ary operation on $\mathbb{N}$.)

As a formal language,
the language of arithmetic also has many unintended models.
For example,
consider the model with domain $\{ Athens, Berlin \}$
and an interpretation function that maps
`0' to Athens,
`+' and `$\times$' to the function that maps any pair of cities to Athens,
`$s$' to the function that maps each city to itself,
and
`<' to the empty relation.
In this model,
`$s(0) + s(s(0)) = s(s(s(0)))$' is true,
but so is `$s(0) = 0$'.

xxx exercise.

We might hope that by laying down sufficiently many postulates in the formal language,
we can rule out all such unintended models.
For example,
if a formal theory of arithmetic contains `$s(0) \ne 0$'
then the above model is no longer a model of the theory.

Isomorphism.

Sometimes this works.
Linear order.



and `$s$
for example,


formal theory of arithmetic would now contain statements like
$s(0) + s(s(0)) = s(s(s(0)))$
and $\forall x (x < x+1)$.
These narrow down the interpretation of the nonlogical symbols.


We might hope that there is some 


That's why textbooks on mathematical logic often identify models with mathematical structures.

Careful: when specifying the algebra, we don't use the first-order object
language. So '+' here is metalanguage. The object language may also have a '+'
symbol or a '0' symbol, but they are not treated as logical. So their meaning
isn't fixed. When we talk about logical entailment, we need to consider any
possible way of interpreting '0' and '+'.

E.g, D = \{ Paris, Rome, Canberra \}, +(x,y) = Paris, $\times$(x,y) = Rome, 0 = Canberra.

Note that, technically, we don't allow for languages with only one or two individual constants.
We need an unending supply of constants for two reasons.
One, the interpretation of $\forall x A$ appeals to $A(x/c)$,
where $c$ is a constant that doesn't occur in $A$.
The interpretation of $\forall x \forall y A$ therefore involves two constants, and so on.
But it doesn't matter what $\mathcal{M}$ assigns to these further constants.
This is also true for the other reason why we need extra constants:
in our calculus,
we often need to reason from $\forall x A(x)$ to $A(c)$,
where $c$ is a new constant,
so that if we have derived $B(c)$ from $A(c)$,
we can later apply Gen to get $\forall x B(x)$.
Here, too, the extra constants only serve a Hilfsfunktion.
They aren't used to pick out a particular object.
(Such constants are historically called \emph{eigenvariables}.)



% ** Isomorphic structures

\begin{definition}{Isomorphic Structures}{isomorphic}
  Structures $\mathcal{A}$ and $\mathcal{B}$ are \emph{isomorphic} if there is a bijection $f$ from the domain of $\mathcal{A}$ to the domain of $\mathcal{B}$ such that xxx
\end{definition}

Clearly, truth at a structure is preserved under isomorphisms. A full proof is in BBJ pp.140ff. Might move to next chapter?

\begin{exercise}
Show that isomorphisms are equivalence relations
\end{exercise}


Compactness is a purely model-theoretic fact.



Now return to first-order logic.
Suppose we want to formalize reasoning about, say, the natural numbers,
in a suitable first-order language $\L$.
This language might have a constant `0',
function symbols `+' and `$\times$',
and so on.
We can say things like
\[
  \forall x (x + 0 = x)
\]
or
\[
  \forall x (x \times 0 = x).
\]
Intuitively,
the first sentence is true and the second false.
But wait!
Officially,
truth and falsity are defined only relative to a model.
It is easy to construct a model in which the second sentence is true.

\begin{exercise}
  Do it.
\end{exercise}

Logic abstracts away from the meaning of non-logical expressions.
`0', `+', `$\times$' are non-logical.
Neither of the two sentences is \emph{logically true},
true in all models.

Still,
there's a good sense in which the first sentence is true and the second false.
When we use the language,
we have a particular interpretation in mind.
We use `0' for the number zero,
`+' for addition,
`$\times$' for multiplication.
We also assume that
the quantifiers range over the set of natural numbers.
So understood,
the first sentence says that adding zero to any number yields that same number,
and the second says that multiplying any number by zero yields that same number.

We'll say that \emph{intended model} of the language $\L$ is the model $\mathcal{N}$
whose domain is the set of natural numbers,
and in which `0' refers to zero,
`+' to addition,
and `$\times$' to multiplication.

We might hope that we can rule out non-intended models by laying down some postulates in the language.
For example,
we can rule out models with only one object by the following postulate:

\[
  \exists x \exists y (x \not= y).
\]

We can similarly find a postulate that rules out models with only two objects.
And so on.

there are at least two objects.

of course, there will always be non-intended models because the nature of the elements in D doesn't matter.
That is, given a model, I can define a new model in which the number 7 is swapped by Caesar etc.
The best we can hope for is to narrow the models down to isomorphism.

Define Isomorphism.

Define elementary equivalence.

Proof sketch: isomorphic models are elementarily equivalent.

If we had the converse -- which we don't --
we'd know that non-isomorphic models can always be distinguished by some sentence.
So that we can always rule out unintended non-isomorphic models by laying down another postulate,
although we might need infinitely many postulates, but never mind that for now.
In fact,
for present purposes we can consider whether it would suffice to lay down
as postulate every sentence in the language that's true in the intended model.

Definition: If M is a model for a language L, the /theory of M/ is the set of L-sentences A with M \models A.

(Note that elementary equivalent models are precisely models with the same theory.)

Consider the language of arithmetic.
Its intended model is called $\mathcal{A}$.
Are all models of the theory of arithmetic isomorphic to the intended model?

No.
We can show this by compactness.
First, extend the language by a new constant $c$.
Add $c \not= 0$ and $c \not= 1$ etc.
Every finite subset is satisfiable in a model based on $\mathcal{A}$,
interpreting $c$ as a sufficiently large number.
So the whole theory is satisfiable in some model $M$.
This model must have an extra element besides the numbers.
Now discard the interpretation of $c$ from the model.
(This is called the \emph{reduct} of $M$ to the original language.)
The resulting model is still a model of $Th(\mathcal{A})$.
But it has an extra element.
It is called a \emph{non-standard model of arithmetic}.
And yet it is elementarily equivalent to $\mathcal{A}$!
There is no sentence in the original language that distinguishes the two models.

\begin{exercise}
  Show by compactness that if $\Gamma$ is a set of first-order sentences with arbitrarily large finite models,
  then $\Gamma$ has an infinite model.
\end{exercise}

The Skolem-Loewenheim theorem makes things much worse.


Are these sentences true?


The second sentence


Consider $\exists x \exists y(x\not=y)$.
Any model of this must have at least size 2.
Its negation only has models with size 1.

\begin{exercise}
Find a sentence with model size at least 3 and at most 2.
\end{exercise}

Here is a set of sentences with only infinite models:

   xxxx [e.g. BBJ 138]




Corollary: Every satisfiable set of sentences has a model whose domain is a set of natural numbers.






\begin{theorem}{(Downward) Löwenheim-Skolem}{lowenheim-skolem}
  If a set $\Gamma$ of countable first-order sentences has a model,
  then it has a countable model.
\end{theorem}

\begin{proof}
  The Henkin model $M_{\Gamma}$ for $\Gamma$ is countable,
  as its domain consists of equivalence classes of closed terms in the language,
  of which (by assumption) there are countably many.
\end{proof}

\begin{theorem}{Upward Löwenheim-Skolem}{lowenheim-skolem}
  If a set $\Gamma$ of first-order sentences has an countably infinite model,
  then it has a model at least as large as any larger cardinality.
\end{theorem}

\begin{proof}
  Let $\kappa$ be any uncountable cardinal.
  Expand the language of $\Gamma$ by $\kappa$ new individual constants.
  For each pair of these constants $c_{i}$ and $c_{j}$,
  add the sentence $c_{i} \neq c_{j}$ to $\Gamma$,
  thereby creating the set $\Gamma^{+}$.
  If $\Gamma$ has a countably infinite model $M$,
  then all finite subsets of $\Gamma^{+}$ are satisfied in the same model,
  with the (finitely many) new constants in the set interpreted as distinct objects in the domain of $M$.
  By the Compactness Theorem,
  $\Gamma^{+}$ has a model $M^{+}$.
  All the $\kappa$ new constants denote distinct objects in this model.
  So the model has at least $\kappa$ objects.
  If we restrict $M^{+}$ to the original language of $\Gamma$,
  we get a model of $\Gamma$ with at least $\kappa$ objects.
\end{proof}

The downward theorem, as stated, was first proved by Thoralf Skolem in 1920,
building on an incomplete 1915 sketch by Leopold Löwenheim.
The upward theorem was proved by Alfred Tarski in 1935,
who also showed that one can find a model whose cardinality is exactly $\kappa$.

Let's think about what these theorems mean,
starting with compactness.
It is easy to find cases where
a conclusion is entailed by infinitely many premises,
but not by any finite subset of those premises.
For example,
from the premises:

\begin{quote}
  I like the number 0;\\
  I like the number 1;\\
  I like the number 2;\\
  \ldots
\end{quote}

and so on, for all natural numbers,
one can infer

\begin{quote}
  I like all natural numbers.
\end{quote}

But this conclusion is not entailed by any finite subset of the premises.
Similarly,
one can infer that
some person X is not an ancestor of some person Y
from the assumptions that

\begin{quote}
  X is not a parent of Y;\\
  X is not a parent of a parent of Y;\\
  X is not a parent of a parent of a parent of Y;\\
  \ldots
\end{quote}

and so on, but not from any finite subset of those assumptions.

The compactness theorem shows that
this kind of situation never arises for \emph{first-order logical entailment}.
The two inference just describe are valid,
but their validity depends on the meaning of non-logical terms:
`1', `2', `3', \ldots, and `number' in the first case;
`parent' and `ancestor' in the second.
From the compactness of first-order logic,
it immediately follows that
one can't define these concepts
using only the connectives, the first-order quantifiers, and identity.

What if we declare the relevant terms to be logical?
Let's see what this would mean for a formalized version of the first inference.
Take a first-order language
with infinitely many constants `0', `1', `2', \ldots, and predicate symbols `$N$' and `$L$'.
The premises can then be formulated as: $L0$, $L1$, $L2$, \ldots.
The conclusion is: $\forall x (Nx \to Lx)$.
Obviously,
the conclusion is not entailed by the premises in the ordinary sense of first-order logical entailment:
we can easily find an interpretation of `0', `1', `2', \ldots, `N', and `L'
that makes the premises true and the conclusion false.
But now the idea is that we can define a new kind of entailment
-- call it ``arithmetical entailment'' --
by fixing the interpretation of the `0', `1', `2', \ldots, and `$N$':
`0' must denote the number zero,
`1' the number one, \ldots,
and the extension of `$N$' must be the set of natural numbers.
Some premises $\Gamma$ ``arithmetically entail'' a conclusion $A$ if
any model of $\Gamma$ that conforms to these rules is a model of $A$.
Now $\{ L1, L2, \ldots \}$ arithmetically entails $\forall x (Nx \to Lx)$.
But no finite subset of $\{ L0, L1, \ldots \}$ arithmetically entails $\forall x (Nx \to Lx)$.
So arithmetical entailment is not compact.
Since compactness is a consequence of soundness and completeness,
it follows that
there can't be a sound and complete proof system for arithmetical validity.
That is,
there is no way to spell out axioms and rules of inference that define a proof relation $\proves_{\mathcal{A}}$ so that
$\Gamma \proves_{\mathcal{A}} A$ iff $\Gamma$ arithmetically entails $A$
-- as long as proofs remain finite.

\begin{exercise}
  Explain how the argument can be run with a single non-arithmetical constant $c$ instead of the predicate symbol $L$.
\end{exercise}

\begin{exercise}
  Consider the ``proof system'' that has all arithmetical truths as axioms, in addition to the axioms and rules of first-order predicate calculus. Can all arithmetical truths be proved in this system? Why doesn't it follow that the system is complete?
\end{exercise}

Thus compactness reveals a deep expressive limitation of first-order logic.
It shows,
for example,
that one cannot express the concept of finiteness.
That is,
there is no first-order sentence that is true in all and only the models with a finite domain.
[Explain.]

\begin{exercise}
  Can you find a sentence that is only true in infinite models?
  Can you find a sentence that is true in all and only the infinite models?
\end{exercise}

Here's another example, from the theory of orders.

A relation $\preceq$ on a set $D$ is a \emph{linear order} if it is

- \emph{reflexive}: for every $x$ in $D$, $x \preceq x$;
- \emph{transitive}: if $x \preceq y$ and $y \preceq z$ then $x \preceq z$;
- \emph{anti-symmetric}: if $x \preceq y$ and $y \preceq x$ then $x=y$;
- \emph{total}: for every $x,y$ in $D$, either $x \preceq y$ or $y \preceq x$.

A linear order $\preceq$ is a \emph{well-order} if every non-empty subset of $D$ has a $\preceq$-least element.
That is,
every non-empty subset $S$ of $D$ has some element $x \in S$ such that for every $y \in S$, $x \preceq y$.
For example,
the less-than-or-equal relation $\leq$ on the natural numbers $\mathbb{N}$ is a well-order:
any set of natural numbers has a least element.

Obviously,
the conditions of reflexivity, anti-symmetry, and totality can be expressed in first-order logic.
So we one can write down a set of first-order sentences with a predicate symbol `$\preceq$'
that is true in a model iff the model interprets `$\preceq$' as a linear order on its domain.
We say that these sentences \emph{axiomatize} the concept of a linear order.
Can we also axiomatize the concept of a well-order?

The answer is no, because of compactness.
Suppose for reductio that some set of first-order sentences $\Gamma$
is true in a model iff it interprets `$\preceq$' as a well-order.
In particular,
$\Gamma$ is true in the model $M$ whose domain are the natural numbers $\mathbb{N}$ and
that interprets `$\preceq$' as the less-than-or-equal relation on $\mathbb{N}$.
Now let $c_{1},c_{2},\ldots$ be a list of individual constants.
Construct an extension $\Gamma^{+}$ of $\Gamma$ by adding,
for each $c_{i}$,
the sentence
\[
  c_{i+1} \preceq c_{i} \land c_{i+1} \not= c_{i},
\]
which we may abbreviate as
\[
  c_{i+1} \prec c_{i}.
\]
Every finite subset of $\Gamma^{+}$ is still satisfiable:
it is true in the model $M$ just mentioned.
By compactness, $\Gamma^{+}$ has a model.
But that model must have an infinite descending chain $\ldots c_{i+1} \prec c_{i} \ldots \prec c_{1}$.
So it is not a well-order.

Compactness is a fundamental theorem in model theory.
Like Gödel,
we've derived it from completeness,
but note that it has nothing to do with proofs.
It's a purely model-theoretic result,
that can also be proved without any reference to proofs.

In fact,
we can use a version of our Henkin proof of Completeness.
Call a set $\Gamma$ \emph{finitely satisfiable} if every finite subset of $\Gamma$ is satisfiable.
Assume that $\Gamma$ is finitely satisfiable.
We would show that $\Gamma$ can be extended to a maximal finitely satisfiable set $\Gamma^{+}$
in a language $\L^{+}$ with new constant symbols.
The Henkin model of $\Gamma^{+}$ is then a model of $\Gamma$.


\begin{exercise}
  A proof system $\proves$ is called \emph{weakly complete} if it can prove all validities:
  if $\entails A$ then $\proves A$.
  In the previous section,
  we've shown that first-order logic is \emph{strongly complete}:
  if $\Gamma \entails A$ then $\Gamma \proves A$.
  Strong completeness obviously implies weak completeness.
  We've seen that it also implies compactness.
  Show that, conversely, weak completeness and compactness together imply strong completeness,
  given that $\proves$ has the properties ID and MP.

  % Answer:
  % Suppose Γ ⊨ φ.
  % So Γ ∪ {¬φ} is unsatisfiable.
  % By compactness, some finite Γ₀ ⊆ Γ makes Γ₀ ∪ {¬φ} unsatisfiable.
  % Let ψ be the conjunction of the formulas in Γ₀. Unsatisfiability of Γ₀ ∪ {¬φ} is the same as validity of ψ → φ. Hence ⊨ ψ → φ.
  % By weak completeness, ⊢ ψ → φ.
  % By ID Γ ⊢ ψ.
  % So by MP, Γ ⊢ φ.
\end{exercise}

\begin{exercise}
  Compactness is an immediate consequence of Completeness.
  Can you argue in the other direction from Compactness to Completeness?
\end{exercise}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "logic3.tex"
%%% End:
